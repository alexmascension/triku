{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing other feature selection methods with triku\n",
    "In this notebook we will compare the performance of triku, compared to other methods. \n",
    "\n",
    "The methods that will be compared will be the following:\n",
    "* Select genes with highest variance.   \n",
    "* Scanpy's `sc.pp.highly_variable_genes`: It is based on Seurat's `vst` method, so they should return similar results.\n",
    "* scry `devianceFeatureSelection()`. This method is featured as the feature selection for Irizarry's GLM-PCA paper (https://doi.org/10.1186/s13059-019-1861-6). From its description, it computes a deviance statistic for each row feature for count data based on a multinomial null model that assumes each feature has a constant rate. Features with large deviance are likely to be informative. Uninformative, low deviance features can be discarded to speed up downstream analyses and reduce memory footprint. The `fam`parameter will be set to `binomial`, the default.\n",
    "* M3Drop, which has two main functions:\n",
    "    * NBDrop: the NBDrop model assumes proportion of zeros follows a Michaelis-Menten model. Then the Michaelis-Menten parameter $K$ is fitted. For each gene, its parameter $K_i$ is compared to $K$ using a $Z$-test, which returns the selected genes.\n",
    "    * NBUmi: The procedure is similar to above, although the equation to fit now is a negative binomial model,  and the selection of genes is then done using a $Z$-test.\n",
    "* `BrenneckeGetVariableGenes` fits a function between CV$^2$ and mean expression. \n",
    "\n",
    "With the exception of scanpy and triku, the rest of functions are set on $R$. We will use jupyter's `%%R` magic command, and `anndata2ri` to transform `annData` into `SingleCellExperiment` objects, and we will generate the functions to accept that annData and return the list of selected features. The functions have to be set up in notebook, and cannot be externalized. \n",
    "\n",
    "M3Drop requires a normalization step, which will be done in-situ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triku as tk\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as spr\n",
    "import scipy.stats as sts\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bokeh.io import show, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearColorMapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ARS\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "# Selection of palettes for cluster coloring, and scatter values\n",
    "from comparing_feat_sel import clustering_binary_search\n",
    "from palettes_and_cmaps import magma, bold_and_vivid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.rinterface_lib.callbacks, logging\n",
    "from rpy2.robjects import pandas2ri\n",
    "import anndata2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore R warning messages\n",
    "#Note: this can be commented out to get more verbose R output\n",
    "rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Automatically convert rpy2 outputs to pandas dataframes\n",
    "anndata2ri.activate()\n",
    "pandas2ri.activate()\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext rmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load all the R libraries we will be using in the notebook\n",
    "library(M3Drop) # Depends on r-foreing (conda-forge) and Hmisc and reldist (install.packages)\n",
    "library(scry) # If R < 4, launch commit 9f0fc819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + '/exports/comparisons/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "run_scry <- function(sce){ #adata\n",
    "    adata_ret = devianceFeatureSelection(sce, nkeep=dim(sce)[1])\n",
    "    return(adata_ret) #returns adata with stats on .var\n",
    "} \n",
    "\n",
    "\n",
    "run_brennecke <- function(sce){ #df\n",
    "    res_df <- BrenneckeGetVariableGenes(sce, suppress.plot=TRUE, fdr=100)\n",
    "    return(res_df) # returns sorted df with genes and stats\n",
    "}\n",
    "\n",
    "\n",
    "run_M3Drop <- function(sce){\n",
    "    norm <- M3DropConvertData(sce, is.counts=TRUE)\n",
    "    DE_genes <- M3DropFeatureSelection(norm, suppress.plot=TRUE, mt_threshold=50)\n",
    "    return(DE_genes) # returns sorted df with genes and stats\n",
    "    \n",
    "}\n",
    "\n",
    "run_NBumi <- function(sce){\n",
    "    count_mat <- NBumiConvertData(sce, is.counts=TRUE)\n",
    "    DANB_fit <- NBumiFitModel(count_mat)\n",
    "    NBDropFS <- NBumiFeatureSelectionCombinedDrop(DANB_fit, suppress.plot=TRUE, qval.thresh=10)\n",
    "    return(NBDropFS)  # returns sorted df with genes and stats\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scanpy(adata):\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    ret = sc.pp.highly_variable_genes(adata, n_top_genes=len(adata), inplace=False)\n",
    "    df = pd.DataFrame(ret)\n",
    "    df =  df.set_index(adata.var_names)\n",
    "    return df # returns df with stats\n",
    "\n",
    "def run_variable(adata):\n",
    "    if spr.issparse(adata.X):\n",
    "        std = adata.X.power(2).mean(0) - np.power(adata.X.mean(0), 2) \n",
    "        std = np.asarray(std).flatten()        \n",
    "    else:\n",
    "        std = adata.X.std(0)\n",
    "        \n",
    "    return std #returns vector with order as var_names \n",
    "\n",
    "def run_triku(adata, seed):\n",
    "    adata_copy = adata.copy()\n",
    "    tk.tl.triku(adata_copy, n_comps=30, n_windows=100, random_state=seed, verbose='error')\n",
    "    return adata_copy.var['emd_distance'] #pd series with distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ORDENAR Y RELLENAR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_feature_ranking(adata, title_prefix, ):\n",
    "    \"\"\"\n",
    "    Create a dataframe with the ranking of features, and another one with the feature values. The adata must be the raw\n",
    "    adata. From that we will create a adata_df necessary for some R methods.\n",
    "    The adata will include:\n",
    "    - Triku with 10 seeds 'triku_SEEDN'\n",
    "    - Scanpy's HVG 'scanpy'\n",
    "    - Std 'std'\n",
    "    - scry 'scry'\n",
    "    - brennecke 'brennecke'\n",
    "    - M3Drop 'm3drop'\n",
    "    - NBumi 'nbumi'\n",
    "    \n",
    "    After each method is run, we will fill the dataframe values, with the values of the metrics used for feature selection, \n",
    "    and the dataframe of rankings with the rankings based on the returned value (0, 1, 2, etc.). \n",
    "    We create two separate dataframes because the df with values might be reserved for other purposes. The rank dataframes is interesting\n",
    "    because the values on the values dataframe have different argsort orders depending on the column (M3drop and NBumi direct, rest reverse).\n",
    "    \"\"\"\n",
    "    \n",
    "    adata = adata.copy()\n",
    "#     sc.pp.subsample(adata, 0.05)\n",
    "    sc.pp.filter_genes(adata, min_cells=1)\n",
    "    sc.pp.filter_cells(adata, min_genes=1)\n",
    "    try:\n",
    "        adata.X = np.asarray(adata.X.todense())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "    adata.obs['groupn'] = adata_groups\n",
    "    adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)\n",
    "        \n",
    "    adata_short = sc.AnnData(X = adata.X) # we have to create a clean adata because some column break Rpush\n",
    "    adata_short.var_names, adata_short.obs_names = adata.var_names, adata.obs_names\n",
    "    %Rpush adata_df\n",
    "    %Rpush adata_short\n",
    "    \n",
    "    \n",
    "    index, columns = adata.var_names, [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi']\n",
    "    df_values, df_ranks = pd.DataFrame(index=index, columns=columns), pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    for i in range(10):\n",
    "        df_emd_distance = run_triku(adata, i)\n",
    "        df_values.loc[df_emd_distance.index, f'triku_{i}'] = df_emd_distance.values\n",
    "        \n",
    "    \n",
    "    scanpy_ret = run_scanpy(adata)\n",
    "    df_values.loc[scanpy_ret.index, 'scanpy'] = scanpy_ret['dispersions_norm'].values\n",
    "    \n",
    "    std_ret = run_variable(adata)\n",
    "    df_values.loc[:, 'std'] = std_ret\n",
    "    \n",
    "    scry_ret = %R run_scry(adata_short)\n",
    "    df_values.loc[scry_ret.var.index, 'scry'] = scry_ret.var['binomial_deviance'].values\n",
    "    \n",
    "    brennecke_ret = %R run_brennecke(adata_df)\n",
    "    df_values.loc[brennecke_ret.index, 'brennecke'] = brennecke_ret['effect.size'].values\n",
    "    \n",
    "    M3Drop_ret = %R run_M3Drop(adata_df)\n",
    "    df_values.loc[M3Drop_ret.index, 'm3drop'] = M3Drop_ret['q.value'].values\n",
    "    \n",
    "    NBumi_ret = %R run_NBumi(adata_df)\n",
    "    df_values.loc[NBumi_ret.index, 'nbumi'] = NBumi_ret['q.value'].values\n",
    "    \n",
    "    # Now we will fill df_ranks with an argsort !!!!! M3DROP and NBumi is not [::-1] because they are q-values \n",
    "    for col in [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke']:\n",
    "        df_ranks[col] = df_values[col].values.argsort()[::-1].argsort()\n",
    "    for col in ['m3drop', 'nbumi']:\n",
    "        df_ranks[col] = df_values[col].values.argsort().argsort() # double argsort to return the rank!\n",
    "    \n",
    "    df_ranks.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_ranks.csv')\n",
    "    df_values.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_values.csv')\n",
    "    \n",
    "    return df_values, df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deprob in tqdm([0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]):\n",
    "    try:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "    except:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    create_df_feature_ranking(adata_deprob, f'scatter_{deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARI - Random datasets\n",
    "Using ARI on random datasets is a measure to assess the effectiveness of the feature selection. Random datasets were prepared with different degrees of differentially expressed gene probability, so that we can compare the leiden clusterign solution with the 9 populations. Triku can be run with different seeds, but the rest of methods are deterministic. However, leiden clustering in all cases can be run with a seed. Therefore, we are going to run all processes with 10 seeds (although the deterministic processes will be run once).\n",
    "\n",
    "To apply the ARI we need to run leiden with as many clusters as scatter populations. Since leiden runs on resolution, we need to adjust the resolution parameter to match the number of clusters. To do that we are going to implement a binary search-like algorithm. We will start with resolutions 0.3 and 2. If any of those yields the clusters, done. Else, find the midpoint, run the clustering, and if the clustering yields the number of populations, stop. Else, set the upper or lower resolution to the one that makes the desired number of clusters to be in the middle. This algorithm will try at most 5 times (it gets to resolution differences of ~0.05, which is fair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatter_dir = os.path.dirname(os.getcwd()) + '/data/splatter/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a dictionary where each element is a dataframe of ARIs run with each DE probability. The \n",
    "adata is of dims n_seeds x n_methods. Therefore, cell i,j will have the ARI between the populations and the optimal leiden clustering.\n",
    "\n",
    "We are going to run the calculations with different feature numbers. To save time, we are going to use a high feature number (5000), store the selected features in places, and for future callings, use the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_deprob, dict_features = {}, {}\n",
    "n_seeds, n_feats = 10, 200\n",
    "min_res, max_res, max_depth = 0.1, 2, 6\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.01\n",
    "\n",
    "adata = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "sc.pp.filter_genes(adata, min_cells=1)\n",
    "sc.pp.filter_cells(adata, min_genes=1)\n",
    "sc.pp.subsample(adata, fraction=0.55)\n",
    "adata.X = np.asarray(adata.X.todense())\n",
    "adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "adata.obs['groupn'] = adata_groups\n",
    "adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)\n",
    "%Rpush adata_df n_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NBumi_feats = %R run_NBumi(adata_df, n_feats)\n",
    "scanpy_feats = run_scanpy(adata, n_feats)\n",
    "triku_feats, adata_triku = run_triku(adata, n_feats, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [triku_feats, scanpy_feats, NBumi_feats]:\n",
    "    adata_plot = adata.copy()\n",
    "    sc.pp.log1p(adata_plot)\n",
    "    adata_plot.var['highly_variable'] = [i in f for i in adata_plot.var_names]\n",
    "    sc.pp.pca(adata_plot, use_highly_variable=True)\n",
    "    sc.pp.neighbors(adata_plot, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "\n",
    "    \n",
    "    c_f, res = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), f)\n",
    "    print(f'{res} || ARI :',  ARS(c_f, adata_groups))\n",
    "    \n",
    "    adata_plot.obs['leiden'], adata_plot.obs['groups'] = c_f, adata_groups\n",
    "    sc.tl.umap(adata_plot)\n",
    "    sc.pl.umap(adata_plot, color=['leiden', 'groups'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [triku_feats, scanpy_feats, NBumi_feats]:\n",
    "    adata_plot = adata.copy()\n",
    "    adata_plot.var['highly_variable'] = [i in f for i in adata_plot.var_names]\n",
    "    sc.pp.pca(adata_plot, use_highly_variable=True)\n",
    "    sc.pp.neighbors(adata_plot, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "\n",
    "    \n",
    "    c_f, res = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), f, apply_log=False)\n",
    "    print(f'{res} || ARI :',  ARS(c_f, adata_groups))\n",
    "    \n",
    "    adata_plot.obs['leiden'], adata_plot.obs['groups'] = c_f, adata_groups\n",
    "    sc.tl.umap(adata_plot)\n",
    "    sc.pl.umap(adata_plot, color=['leiden', 'groups'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scanpy = adata.copy()\n",
    "adata_scanpy.var['highly_variable'] = [i in scanpy_feats for i in adata_plot.var_names]\n",
    "sc.pp.pca(adata_scanpy, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_scanpy, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_scanpy)\n",
    "\n",
    "adata_triku = adata.copy()\n",
    "adata_triku.var['highly_variable'] = [i in triku_feats for i in adata_plot.var_names]\n",
    "tk.tl.triku(adata_triku, n_features=n_features, n_windows=100, verbose='error')\n",
    "sc.pp.pca(adata_triku, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_triku, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_triku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_genes = list(set(triku_feats) & set(scanpy_feats))\n",
    "triku_only =list(set(triku_feats) - set(scanpy_feats))\n",
    "scanpy_only = list(set(scanpy_feats) - set(triku_feats))\n",
    "print(len(common_genes), len(triku_only), len(scanpy_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scanpy, color=['groupn'] + scanpy_only, legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scanpy, color=['groupn'] + triku_only, legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = []\n",
    "labels = []\n",
    "alpha = [0.8 if i in triku_feats or i in scanpy_feats else 0.4 for i in adata.var_names]\n",
    "\n",
    "for i in adata.var_names:\n",
    "    if i in triku_feats and i in scanpy_feats:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif i in triku_feats:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Triku')\n",
    "    elif i in scanpy_feats:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Scanpy')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh = pd.DataFrame({\n",
    "    'm': np.log10(adata_triku.X.mean(0)),\n",
    "    'z': (adata_triku.X == 0).sum(0) / adata_triku.shape[0],\n",
    "    'n': adata_triku.var_names.values,\n",
    "    'd': adata_triku.var[\"emd_distance\"],\n",
    "    'color': color, 'label':labels, 'alpha':alpha\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@d')])\n",
    "\n",
    "p.scatter('m', 'd', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_gene(adata, gene):\n",
    "    mean_exp_val = []\n",
    "    \n",
    "    groups = sorted(list(set(adata.obs['groupn'].values)))\n",
    "    dict_argwhere = {group: np.argwhere(adata.obs['groupn'].values == group) for group in groups}\n",
    "    exp_gene = adata_triku[:, gene].X.ravel()\n",
    "    \n",
    "    for g in groups:\n",
    "        exp_group = np.sort(exp_gene[dict_argwhere[g]].ravel())\n",
    "        mean_exp_val.append(np.mean(exp_group[: int(0.90 * len(exp_group))])) # for genes with small expression it may amplify noise\n",
    "        \n",
    "    \n",
    "    mean_exp_val_1 = np.array(mean_exp_val)/sum(mean_exp_val)\n",
    "    \n",
    "#     info = sts.entropy(mean_exp_val_1) / np.log(len(groups))\n",
    "    info = max(np.sort(mean_exp_val_1)[3] - np.sort(mean_exp_val_1)[0], np.sort(mean_exp_val_1)[-1] - np.sort(mean_exp_val_1)[-4])\n",
    "    \n",
    "    return info, mean_exp_val_1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'Gene13823'\n",
    "\n",
    "ent, arr = get_info_gene(adata_triku, gene)\n",
    "print(ent)\n",
    "print(list(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_triku, color=['groupn', gene], legend_loc='on data', cmap=magma)\n",
    "sc.pl.umap(adata_scanpy, color=['groupn', gene], legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_deprob, dict_features = {}, {}\n",
    "n_seeds, n_feats = 10, 5000\n",
    "min_res, max_res, max_depth = 0.1, 2, 6\n",
    "\n",
    "for deprob in [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]:\n",
    "    adata = sc.read_loom(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    adata.X = np.asarray(adata.X.todense())\n",
    "    adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "    adata_df = pd.DataFrame(np.asarray(adata.X.T.todense()), index=adata.var_names, columns=adata.obs_names)\n",
    "    \n",
    "    %Rpush adata_df n_feats\n",
    "    \n",
    "    df = pd.DataFrame(index = np.arange(n_seeds), columns=['triku', 'var', 'scanpy', 'scry', 'brennecke', 'M3Drop', 'NBUmi'])\n",
    "    \n",
    "    for seed in np.arange(n_seeds):\n",
    "        if seed == 0:\n",
    "            triku_feats = run_triku(adata, n_feats, seed)\n",
    "            var_feats = run_variable(adata, n_feats)\n",
    "            scanpy_feats = run_scanpy(adata, n_feats)\n",
    "            NBumi_feats = %R run_NBumi(adata_df, n_feats)\n",
    "            scry_feats = %R run_scry(adata_df, n_feats)\n",
    "            brennecke_feats = %R run_brennecke(adata_df, n_feats)\n",
    "            M3Drop_feats = %R run_M3Drop(adata_df, n_feats)\n",
    "            \n",
    "            dict_features[f'triku_{seed}'], dict_features['var'], dict_features['scanpy'] = triku_feats, var_feats, scanpy_feats\n",
    "            dict_features['scry'], dict_features['brennecke'], dict_features['M3Drop'], dict_features['NBUmi'] = scry_feats, brennecke_feats, M3Drop_feats, NBumi_feats\n",
    "        \n",
    "        else:\n",
    "            triku_feats = run_triku(adata, n_feats, seed)\n",
    "            dict_features[f'triku_{seed}'] = triku_feats\n",
    "            \n",
    "        # Run clustering with each method and get ARI\n",
    "        c_triku = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), triku_feats)\n",
    "        df.loc[seed, 'triku'] = ARI(c_triku, adata_groups)\n",
    "        c_var = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), var_feats)\n",
    "        df.loc[seed, 'var'] = ARI(c_var, adata_groups)\n",
    "        c_scanpy = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scanpy_feats)\n",
    "        df.loc[seed, 'scanpy'] = ARI(c_scanpy, adata_groups)\n",
    "\n",
    "        c_scry = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scry_feats)\n",
    "        df.loc[seed, 'scry'] = ARI(c_scry, adata_groups)\n",
    "        c_brennecke = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), brennecke_feats)\n",
    "        df.loc[seed, 'brennecke'] = ARI(c_brennecke, adata_groups)\n",
    "        c_M3Drop = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), M3Drop_feats)\n",
    "        df.loc[seed, 'M3Drop'] = ARI(c_M3Drop, adata_groups)\n",
    "        c_NBumi = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scry_NBumi)\n",
    "        df.loc[seed, 'NBumi'] = ARI(c_NBumi, adata_groups)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "aaa <- function(a, b, c){\n",
    "    return(a+b+c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 9\n",
    "b = 19\n",
    "c = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i a -i b -i c -o d\n",
    "\n",
    "d = aaa(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush a b c\n",
    "d = %R aaa(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i n -o a\n",
    "\n",
    "a = run_scry(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush df n\n",
    "a = %Rget run_scry(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_loom(splatter_dir + f'/splatter_deprob_{0.01}.loom')\n",
    "adata.obs['groups'] = [i.replace('Group', '') for i in adata.obs['Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.datasets.pbmc3k()\n",
    "sc.pp.filter_genes(adata, min_cells=10)\n",
    "df = pd.DataFrame(np.asarray(adata.X.T.todense()), index=adata.var_names, columns=adata.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i df -o run_M3Drop\n",
    "\n",
    "run_M3Drop <- run_NBumi(df, 1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i adata -o scry_ret -o brennecke_ret -o M3Drop_ret -o NBumi_ret\n",
    "\n",
    "scry_ret <- run_scry(adata, 1500)\n",
    "brennecke_ret <- run_brennecke(adata, 1500)\n",
    "M3Drop_ret <- run_M3Drop(df, 1500)\n",
    "NBumi_ret <- run_NBumi(df, 1500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_M3Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "# Selection of palettes for cluster coloring, and scatter values\n",
    "from palettes_and_cmaps import magma, bold_and_vivid\n",
    "from robustness_functions import run_batch, random_noise_parameter, plot_scatter_parameter, compare_parameter, plot_scatter_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(os.getcwd()) + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/'\n",
    "read_dir = data_dir + 'Ding_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to read you must have: EXACTLY in that manner:\n",
    "# matrix.mtx.gz (you MUST rename it)\n",
    "# features.tsv(you should rename it)\n",
    "# barcodes.tsv (you should rename it)\n",
    "from scipy.io import mmread\n",
    "\n",
    "matrix = mmread('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/matrix.mtx.gz')\n",
    "features = np.loadtxt('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/features.tsv', dtype=str)\n",
    "barcodes = np.loadtxt('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/barcodes.tsv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=matrix.tocsr()).transpose()\n",
    "adata.var_names = features\n",
    "adata.obs_names = barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/meta_combined.txt', sep='\\t', skiprows=[1])\n",
    "adata = adata[meta['NAME'].values]\n",
    "adata.obs['method'] = meta['Method'].values\n",
    "adata.obs['CellType'] = meta['CellType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1_000\n",
    "a-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_text(read_dir + '/GSE133545_SMARTseq2_human_exp_mat.tsv').transpose()\n",
    "adata.var_names_make_unique()\n",
    "sc.pp.filter_genes(adata, min_cells=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tl.triku(adata, verbose='triku', n_procs=4, knn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_a(a, b): return np.convolve(a, b, )\n",
    "def func_b(a, b): \n",
    "    x = fftconvolve(a, b, )\n",
    "    x[x < 0] = 0\n",
    "    \n",
    "    return x\n",
    "\n",
    "def apply_convolution_read_counts(probs: np.ndarray, knn: int, func) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Convolution of functions. The function applies a convolution using np.convolve\n",
    "    of a probability distribution knn times. The result is an array of N elements (N arises as the convolution\n",
    "    of a n-length array knn times) where the element i has the probability of i being observed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs : np.array\n",
    "        Object with count matrix. If `pandas.DataFrame`, rows are cells and columns are genes.\n",
    "    knn : int\n",
    "        Number of kNN\n",
    "    \"\"\"\n",
    "      \n",
    "    \n",
    "    # We are calculating the convolution of cells with positive expression. Thus, in the first distribution\n",
    "    # we have to remove the cells with 0 reads, and rescale the probabilities.\n",
    "    arr_0 = probs.copy()\n",
    "    arr_0[0] = 0  # TODO: this will fail in log-transformed data\n",
    "    arr_0 /= arr_0.sum()\n",
    "\n",
    "    # We will use arr_bvase as the array with the read distribution\n",
    "    arr_base = probs.copy()\n",
    "    \n",
    "    arr_convolve = func(arr_0, arr_base, )\n",
    "    \n",
    "    for knni in range(2, knn):\n",
    "        arr_convolve = func(arr_convolve, arr_base, )\n",
    "\n",
    "    # TODO: check the probability sum is 1 and, if so, remove\n",
    "    arr_prob = arr_convolve / arr_convolve.sum()\n",
    "\n",
    "    # TODO: if log transformed, this is untrue. Should not be arange.\n",
    "    return np.arange(len(arr_prob)), arr_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_gene = adata.X[:, 377]\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_a, sums_a, times_b, sums_b = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(500, 1000)):\n",
    "    counts_gene = adata.X[:, i]\n",
    "    y_probs = np.bincount(counts_gene.astype(int)) / len(counts_gene)\n",
    "    t = time.time()\n",
    "    apply_convolution_read_counts(y_probs, 50, func_a)\n",
    "    times_a.append(time.time() - t)\n",
    "    sums_a.append(counts_gene.sum())\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    apply_convolution_read_counts(y_probs, 50, func_b)\n",
    "    times_b.append(time.time() - t)\n",
    "    sums_b.append(counts_gene.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.scatter(np.log10(sums_a), np.log10(times_a))\n",
    "plt.scatter(np.log10(sums_b), np.log10(times_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs, len(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = apply_convolution_read_counts(y_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = apply_convolution_read_counts(y_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(((a[1]-b[1])**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "This notebook contains several cells, each explaining its own purpose. The notebook is prepared to run with only one dataset from the Holger dataset. I force the use of this dataset simply because the robustness is easily checked in all cases, and there is no point in generalise the functions to other datasets, at least in this case.\n",
    "\n",
    "**SET THE VARIABLES BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_prep, org = 'QUARTZseq', 'human'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness based on seed\n",
    "Many of the parts from triku require a random seed: PCA calculation (because we use the randomized variant), nearest neighbor calculation, and random matrix generation. Each of those processes interfere with the distances we obtain, either the ones from the original datasets, or the ones from the randomized dataset. Due to that, we have to evaluate this noise, and see how it might affect the genes that are given, even without considering the randomisation.\n",
    "\n",
    "To do that we will study which of the variables affect, or enhance, the noise: kNN calculation, PCA, or dataset randomization (number of windows is not stochastic, so it won't be a factor). To see the effect, we will do two types of plots: variability evaluation, and limit of features achievable given the level of noise.\n",
    "\n",
    "For the first type of plot we will fix a variable and evaluate a range of the other variables (fix kNN and evaluate number of components, and vice-versa). If, for example, we fix kNN, for each of the possible number of components, we will take pairs of dataframes (combinations of two seeds), and compare their corrected and uncorrected distance together. \n",
    "\n",
    "How do we do the comparison: if $d_A$ is a distance from df with seed A, and $d_B$ is the distance in the same gene for B, then the comparison value is: \n",
    "\n",
    "$$\\frac{|d_A - d_B|}{|d_A| + |d_B|}$$\n",
    "\n",
    "This value is range between 0 ($d_A = d_B$) and 1 ($d_A$ and $d_B$ have opposite signs).\n",
    "\n",
    "We plot a swarmplot with three categories: the first 250 features (based on highest distance), 1000 and 5000 features. We should expect less noise on the first features, because they are the ones with more distance. Also, for each option in the swarmplot, the column on the **left are distances without randomization** and the column on the **right are distances with randomization**. We should expect higher noise on distances with randomization, because the random noise from the randomized dataset is highger then the one from the non-randomized one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_0_250 = random_noise_parameter(lib_prep, org, save_dir, 0, 250, what='relative noise', by='knn')\n",
    "df_250_1000 = random_noise_parameter(lib_prep, org, save_dir, 250, 1000, what='relative noise', by='knn')\n",
    "df_1000_5000 = random_noise_parameter(lib_prep, org, save_dir, 1000, 5000, what='relative noise', by='knn')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter(\n",
    "    [df_1000_5000, df_250_1000, df_0_250], \n",
    "    ['1000 - 5000', '250 - 1000', '0 - 250'], \n",
    "    lib_prep, org, by='knn', \n",
    "    title='Noise_distance_from_seed,_kNN', \n",
    "    ylabel=\"$\\\\frac{|d_A - d_B|}{|d_A| + |d_B|}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_0_250 = random_noise_parameter(lib_prep, org, save_dir, 0, 250, what='relative noise', by='pca')\n",
    "df_250_1000 = random_noise_parameter(lib_prep, org, save_dir, 250, 1000, what='relative noise', by='pca')\n",
    "df_1000_5000 = random_noise_parameter(lib_prep, org, save_dir, 1000, 5000, what='relative noise', by='pca')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter(\n",
    "    [df_1000_5000, df_250_1000, df_0_250], \n",
    "    ['1000 - 5000', '250 - 1000', '0 - 250'], \n",
    "    lib_prep, org, by='pca',\n",
    "    title='Noise_distance_from_seed,_PCA',\n",
    "    ylabel=\"$\\\\frac{|d_A - d_B|}{|d_A| + |d_B|}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kNN we see that there is generally a sweet spot between $\\sqrt{N}/2$ and 2$\\sqrt{N}$, with smaller variation in the distances without randomization. As for PCA components, we see that, interestingly, seed noise increases with the number of PCA components. Although paradoxical, it makes sense: first components are less prone to ve variable, and thus it is more difficult to experience noise. In fact, if we consider 3 components, the noise is almost 0 in the non-randomized set of distances, which tells us that the noise in the randomized set of distances arises mainly due to dataset randomization. However, although the noise is small, it does not mean that the selected features are the correct ones! Fewer components means less information from the datasets. We'll see that when we compare festures across components, later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plots we will consider directly the overlapping between features. For each set of features within an overlap range, we will see the percentage of overlap between those features. That is, for example, for kNN = $\\sqrt{N}$ and 100 PCA components, compare the first 250 features between seed 0 and seed 1. Instead of considering features 250 - 1000 and 1000 - 5000, we will consider features 0 - 1000 and 0 - 5000 directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_0_250 = random_noise_parameter(lib_prep, org, save_dir, 0, 250, what='overlap', by='knn')\n",
    "df_0_1000 = random_noise_parameter(lib_prep, org, save_dir, 0, 1000, what='overlap', by='knn')\n",
    "df_0_5000 = random_noise_parameter(lib_prep, org, save_dir, 0, 5000, what='overlap', by='knn')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_0_5000, df_0_1000, df_0_250], \n",
    "   ['0 - 5000', '0 - 1000', '0 - 250'],\n",
    "    lib_prep, org, step=1, by='knn',\n",
    "    palette = 'sunsetmid3', \n",
    "    title='Overlap_of_features_from_seed,_kNN',\n",
    "    ylabel='Overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_0_250 = random_noise_parameter(lib_prep, org, save_dir, 0, 250, what='overlap', by='pca')\n",
    "df_0_1000 = random_noise_parameter(lib_prep, org, save_dir, 0, 1000, what='overlap', by='pca')\n",
    "df_0_5000 = random_noise_parameter(lib_prep, org, save_dir, 0, 5000, what='overlap', by='pca')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_0_5000, df_0_1000, df_0_250], \n",
    "            ['0 - 5000', '0 - 1000', '0 - 250'], lib_prep, org, \n",
    "            step=1, palette = 'sunsetmid3', \n",
    "            by='pca', title='Overlap_of_features_from_seed,_PCA', \n",
    "                      ylabel='Overlap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar trend than in the previous case: low kNN values can be detrimental to the quality of feature selection, although the effect is resolved with a number of kNN between $\\sqrt{N}/2$ and 2$\\sqrt{N}$. Regarding PCA components, there is not much variation, with overlap values higher than 95% at sensible PCA component values near 20-30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness between different parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to compare the overlapping percentage of number of features given different parameter values. As in the previous section, we are foing to fix the number of kNN in $\\sqrt{N}$, number of PCA components in 30, additionally, we are going to fix the two parameters to see changes on number of windows for median correction. \n",
    "\n",
    "The strategy in this case will be the same: consider distance values for each of the parametters, and calculate the overlap between the first N features. For example, when comparing kNN values, we are going to compare the values from $\\sqrt{N}$, seed 0 with $2\\sqrt{N}$ seed 1, seed 2, etc. We will also compare $\\sqrt{N}$ with itself, which has already been done, but which will still be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='overlap', by='knn')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='overlap', by='knn')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='overlap', by='knn')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='overlap', by='knn')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, \n",
    "    palette = 'sunsetmid4', by='knn',\n",
    "    title='kNN_robustness,_overlap', \n",
    "    ylabel='Overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='overlap', by='pca')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='overlap', by='pca')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='overlap', by='pca')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='overlap', by='pca')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, \n",
    "    palette = 'sunsetmid4', by='pca', \n",
    "    title='PCA_robustness,_overlap', \n",
    "    ylabel='Overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='overlap', by='w')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='overlap', by='w')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='overlap', by='w')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='overlap', by='w')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, palette = 'sunsetmid4', by='w', \n",
    "    title='window_robustness,_overlap', \n",
    "    ylabel='Overlap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SACAR COMENTARIOS DE AQUI y HACER WINDOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='correlation', by='knn')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='correlation', by='knn')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='correlation', by='knn')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='correlation', by='knn')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, \n",
    "    palette = 'sunsetmid4', by='knn',\n",
    "    title='kNN_robustness,_correlation', \n",
    "    ylabel='Pearson correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='correlation', by='pca')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='correlation', by='pca')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='correlation', by='pca')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='correlation', by='pca')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, \n",
    "    palette = 'sunsetmid4', by='pca', \n",
    "    title='PCA_robustness,_correlation', \n",
    "    ylabel='Pearson correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_violin_0_500 = compare_parameter(lib_prep, org, save_dir, 0, 500, what='correlation', by='w')\n",
    "df_violin_0_1000 = compare_parameter(lib_prep, org, save_dir, 0, 1000, what='correlation', by='w')\n",
    "df_violin_0_2500 = compare_parameter(lib_prep, org, save_dir, 0, 2500, what='correlation', by='w')\n",
    "df_violin_0_5000 = compare_parameter(lib_prep, org, save_dir, 0, 5000, what='correlation', by='w')\n",
    "\n",
    "# Remember: left = non randomized - right = randomized\n",
    "plot_scatter_parameter([df_violin_0_5000, df_violin_0_2500, df_violin_0_1000, df_violin_0_500], \n",
    "   ['0 - 5000',  '0 - 2500', '0 - 1000', '0 - 500'], \n",
    "    lib_prep, org, step=1, palette = 'sunsetmid4', by='w',\n",
    "    title='window_robustness,_correlation',\n",
    "    ylabel=\"Pearson correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SACAR COMENTARIOS DE AQUI y HACER WINDOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org, lib_preps = 'human', ['SingleNuclei', 'inDrop', '10XV3', 'SMARTseq2', 'CELseq2', 'QUARTZseq',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dicts_dfs = [{}, {}, {}]\n",
    "by, what = 'knn', 'overlap'\n",
    "low_val, mid_val, hi_val = 500, 1500, 2500\n",
    "\n",
    "for lib_prep in lib_preps:\n",
    "    list_dicts_dfs[0][lib_prep] = compare_parameter(lib_prep, org, save_dir, 0, low_val, what=what, by=by)\n",
    "    list_dicts_dfs[1][lib_prep] = compare_parameter(lib_prep, org, save_dir, 0, mid_val, what=what, by=by)\n",
    "    list_dicts_dfs[2][lib_prep] = compare_parameter(lib_prep, org, save_dir, 0, hi_val, what=what, by=by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_datasets(list_dicts_dfs, org, by, figsize=(7, 4),  palette='prism',\n",
    "                           title='', ylabel='', save_dir='robustness_figs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(os.getcwd()) + '/data/'\n",
    "save_dir = os.getcwd() + '/exports/'\n",
    "read_dir = data_dir + 'Holger_CNAG_2019/'\n",
    "\n",
    "\n",
    "for file in os.listdir(read_dir):\n",
    "    if org in file and 'exp_mat' in file and lib_prep in file:\n",
    "        file_in = file\n",
    "        \n",
    "        \n",
    "adata = sc.read_text(read_dir + file_in).transpose()\n",
    "adata.var_names_make_unique()\n",
    "sc.pp.filter_genes(adata, min_cells=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "adata_1 = adata.copy()\n",
    "tk.tl.triku(adata_1, n_comps=30, n_windows=70, \n",
    "            knn=54, random_state=1)\n",
    "\n",
    "dist_uncorrected = adata_1.var['emd_distance_uncorrected'] - adata_1.var['emd_distance_random']\n",
    "n_features = np.sum(adata_1.var['highly_variable'].values)\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "highly_variable_uncorrected = dist_uncorrected > cutoff\n",
    "highly_variable_corrected = adata_1.var['highly_variable']\n",
    "color = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(highly_variable_uncorrected)):\n",
    "    if highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif highly_variable_corrected[i] and not highly_variable_uncorrected[i]:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Corrected')\n",
    "    elif not highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Uncorrected')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh_1 = pd.DataFrame({\n",
    "    'm': np.log10(adata_1.X.mean(0)),\n",
    "    'z': (adata_1.X == 0).sum(0) / adata_1.shape[0],\n",
    "    'n': adata_1.var_names.values,\n",
    "    'd': adata_1.var[\"emd_distance_uncorrected\"],\n",
    "    'e': adata_1.var[\"emd_distance_uncorrected\"] - adata_1.var[\"emd_distance_random\"],\n",
    "    'e_correct': adata_1.var[\"emd_distance\"],\n",
    "    'color': color, 'label':labels\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "adata_2 = adata.copy()\n",
    "tk.tl.triku(adata_2, n_comps=30, \n",
    "            n_windows=100, knn=15, \n",
    "            random_state=1)\n",
    "\n",
    "dist_uncorrected = adata_2.var['emd_distance_uncorrected'] - adata_2.var['emd_distance_random']\n",
    "n_features = np.sum(adata_2.var['highly_variable'].values)\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "highly_variable_uncorrected = dist_uncorrected > cutoff\n",
    "highly_variable_corrected = adata_2.var['highly_variable']\n",
    "color = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(highly_variable_uncorrected)):\n",
    "    if highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif highly_variable_corrected[i] and not highly_variable_uncorrected[i]:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Corrected')\n",
    "    elif not highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Uncorrected')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh_2 = pd.DataFrame({\n",
    "    'm': np.log10(adata_2.X.mean(0)),\n",
    "    'z': (adata_2.X == 0).sum(0) / adata_2.shape[0],\n",
    "    'n': adata_2.var_names.values,\n",
    "    'd': adata_2.var[\"emd_distance_uncorrected\"],\n",
    "    'e': adata_2.var[\"emd_distance_uncorrected\"] - adata_2.var[\"emd_distance_random\"],\n",
    "    'e_correct': adata_2.var[\"emd_distance\"],\n",
    "    'color': color, 'label':labels\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), \n",
    "                                                                                    ('Value', '@e'), \n",
    "                                                                                    ('Unc', '@d')])\n",
    "\n",
    "p.scatter('m', 'e_correct', source=df_bokeh_1,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_right'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), \n",
    "                                                                                    ('Value', '@e'), \n",
    "                                                                                    ('Unc', '@d')])\n",
    "\n",
    "p.scatter('m', 'e_correct', source=df_bokeh_2,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_right'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_seed_1 = adata_1.var['emd_distance']\n",
    "dist_seed_2 = adata_2.var['emd_distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0, Nf = 0, 1000\n",
    "len(np.intersect1d(dist_seed_1.sort_values(ascending=False).index[N0:Nf].values,\n",
    "                dist_seed_2.sort_values(ascending=False).index[N0:Nf].values))/(Nf - N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 2000, 15)\n",
    "y = [len(np.intersect1d(dist_seed_1.sort_values(ascending=False).index[N0:Nf].values,\n",
    "                dist_seed_2.sort_values(ascending=False).index[N0:Nf].values))/(Nf - N0) for Nf in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alex-base] *",
   "language": "python",
   "name": "conda-env-alex-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
