{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing other feature selection methods with triku\n",
    "In this notebook we will compare the performance of triku, compared to other methods. \n",
    "\n",
    "The methods that will be compared will be the following:\n",
    "* Select genes with highest variance.   \n",
    "* Scanpy's `sc.pp.highly_variable_genes`: It is based on Seurat's `vst` method, so they should return similar results.\n",
    "* scry `devianceFeatureSelection()`. This method is featured as the feature selection for Irizarry's GLM-PCA paper (https://doi.org/10.1186/s13059-019-1861-6). From its description, it computes a deviance statistic for each row feature for count data based on a multinomial null model that assumes each feature has a constant rate. Features with large deviance are likely to be informative. Uninformative, low deviance features can be discarded to speed up downstream analyses and reduce memory footprint. The `fam`parameter will be set to `binomial`, the default.\n",
    "* M3Drop, which has two main functions:\n",
    "    * NBDrop: the NBDrop model assumes proportion of zeros follows a Michaelis-Menten model. Then the Michaelis-Menten parameter $K$ is fitted. For each gene, its parameter $K_i$ is compared to $K$ using a $Z$-test, which returns the selected genes.\n",
    "    * NBUmi: The procedure is similar to above, although the equation to fit now is a negative binomial model,  and the selection of genes is then done using a $Z$-test.\n",
    "* `BrenneckeGetVariableGenes` fits a function between CV$^2$ and mean expression. \n",
    "\n",
    "With the exception of scanpy and triku, the rest of functions are set on $R$. We will use jupyter's `%%R` magic command, and `anndata2ri` to transform `annData` into `SingleCellExperiment` objects, and we will generate the functions to accept that annData and return the list of selected features. The functions have to be set up in notebook, and cannot be externalized. \n",
    "\n",
    "M3Drop requires a normalization step, which will be done in-situ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triku as tk\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as spr\n",
    "import scipy.stats as sts\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bokeh.io import show, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearColorMapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ARS\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "# Selection of palettes for cluster coloring, and scatter values\n",
    "from comparing_feat_sel import clustering_binary_search, plot_max_var_x_dataset, plot_max_var_x_method, get_max_diff_gene\n",
    "from palettes_and_cmaps import magma, bold_and_vivid, prism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.rinterface_lib.callbacks, logging\n",
    "from rpy2.robjects import pandas2ri\n",
    "import anndata2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore R warning messages\n",
    "#Note: this can be commented out to get more verbose R output\n",
    "rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Automatically convert rpy2 outputs to pandas dataframes\n",
    "anndata2ri.activate()\n",
    "pandas2ri.activate()\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext rmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load all the R libraries we will be using in the notebook\n",
    "library(M3Drop) # Depends on r-foreing (conda-forge) and Hmisc and reldist (install.packages)\n",
    "library(scry) # If R < 4, launch commit 9f0fc819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + '/exports/comparisons/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following 3 cells we will create the cells that obtain the most relevant features. Since some of the calls are to R, they have to be kept as separate cells. Also, we create the function `create_df_feature_ranking` which creates two dataframes: one with the evaluation values (p-value, emd distance, etc.) of each method, and the second one with the ranking of genes based on those values. These dataframes will be valuable so that we don't have to repeat the calling to the feature selection methods each time we do a graph. `create_df_feature_ranking` is also kept as a cell because it makes some calls to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "run_scry <- function(sce){ #adata\n",
    "    adata_ret = devianceFeatureSelection(sce, nkeep=dim(sce)[1])\n",
    "    return(adata_ret) #returns adata with stats on .var\n",
    "} \n",
    "\n",
    "\n",
    "run_brennecke <- function(sce){ #df\n",
    "    res_df <- BrenneckeGetVariableGenes(sce, suppress.plot=TRUE, fdr=100)\n",
    "    return(res_df) # returns sorted df with genes and stats\n",
    "}\n",
    "\n",
    "\n",
    "run_M3Drop <- function(sce){\n",
    "    norm <- M3DropConvertData(sce, is.counts=TRUE)\n",
    "    DE_genes <- M3DropFeatureSelection(norm, suppress.plot=TRUE, mt_threshold=50)\n",
    "    return(DE_genes) # returns sorted df with genes and stats\n",
    "    \n",
    "}\n",
    "\n",
    "run_NBumi <- function(sce){\n",
    "    count_mat <- NBumiConvertData(sce, is.counts=TRUE)\n",
    "    DANB_fit <- NBumiFitModel(count_mat)\n",
    "    NBDropFS <- NBumiFeatureSelectionCombinedDrop(DANB_fit, suppress.plot=TRUE, qval.thresh=10)\n",
    "    return(NBDropFS)  # returns sorted df with genes and stats\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scanpy(adata):\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    ret = sc.pp.highly_variable_genes(adata, n_top_genes=len(adata), inplace=False)\n",
    "    df = pd.DataFrame(ret)\n",
    "    df =  df.set_index(adata.var_names)\n",
    "    return df # returns df with stats\n",
    "\n",
    "def run_variable(adata):\n",
    "    if spr.issparse(adata.X):\n",
    "        std = adata.X.power(2).mean(0) - np.power(adata.X.mean(0), 2) \n",
    "        std = np.asarray(std).flatten()        \n",
    "    else:\n",
    "        std = adata.X.std(0)\n",
    "        \n",
    "    return std #returns vector with order as var_names \n",
    "\n",
    "def run_triku(adata, seed):\n",
    "    adata_copy = adata.copy()\n",
    "    tk.tl.triku(adata_copy, n_comps=30, n_windows=100, random_state=seed, verbose='error')\n",
    "    return adata_copy.var['emd_distance'] #pd series with distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_feature_ranking(adata, title_prefix, ):\n",
    "    \"\"\"\n",
    "    Create a dataframe with the ranking of features, and another one with the feature values. The adata must be the raw\n",
    "    adata. From that we will create a adata_df necessary for some R methods.\n",
    "    The adata will include:\n",
    "    - Triku with 10 seeds 'triku_SEEDN'\n",
    "    - Scanpy's HVG 'scanpy'\n",
    "    - Std 'std'\n",
    "    - scry 'scry'\n",
    "    - brennecke 'brennecke'\n",
    "    - M3Drop 'm3drop'\n",
    "    - NBumi 'nbumi'\n",
    "    \n",
    "    After each method is run, we will fill the dataframe values, with the values of the metrics used for feature selection, \n",
    "    and the dataframe of rankings with the rankings based on the returned value (0, 1, 2, etc.). \n",
    "    We create two separate dataframes because the df with values might be reserved for other purposes. The rank dataframes is interesting\n",
    "    because the values on the values dataframe have different argsort orders depending on the column (M3drop and NBumi direct, rest reverse).\n",
    "    \"\"\"\n",
    "    \n",
    "    adata = adata.copy()\n",
    "#     sc.pp.subsample(adata, 0.05)\n",
    "    sc.pp.filter_genes(adata, min_cells=1)\n",
    "    sc.pp.filter_cells(adata, min_genes=1)\n",
    "    try:\n",
    "        adata.X = np.asarray(adata.X.todense())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "    adata.obs['groupn'] = adata_groups\n",
    "    adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)\n",
    "        \n",
    "    adata_short = sc.AnnData(X = adata.X) # we have to create a clean adata because some column break Rpush\n",
    "    adata_short.var_names, adata_short.obs_names = adata.var_names, adata.obs_names\n",
    "    %Rpush adata_df\n",
    "    %Rpush adata_short\n",
    "    \n",
    "    \n",
    "    index, columns = adata.var_names, [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi']\n",
    "    df_values, df_ranks = pd.DataFrame(index=index, columns=columns), pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    for i in range(10):\n",
    "        df_emd_distance = run_triku(adata, i)\n",
    "        df_values.loc[df_emd_distance.index, f'triku_{i}'] = df_emd_distance.values\n",
    "        \n",
    "    \n",
    "    scanpy_ret = run_scanpy(adata)\n",
    "    df_values.loc[scanpy_ret.index, 'scanpy'] = scanpy_ret['dispersions_norm'].values\n",
    "    \n",
    "    std_ret = run_variable(adata)\n",
    "    df_values.loc[:, 'std'] = std_ret\n",
    "    \n",
    "    scry_ret = %R run_scry(adata_short)\n",
    "    df_values.loc[scry_ret.var.index, 'scry'] = scry_ret.var['binomial_deviance'].values\n",
    "    \n",
    "    brennecke_ret = %R run_brennecke(adata_df)\n",
    "    df_values.loc[brennecke_ret.index, 'brennecke'] = brennecke_ret['effect.size'].values\n",
    "    \n",
    "    M3Drop_ret = %R run_M3Drop(adata_df)\n",
    "    df_values.loc[M3Drop_ret.index, 'm3drop'] = M3Drop_ret['q.value'].values\n",
    "    \n",
    "    NBumi_ret = %R run_NBumi(adata_df)\n",
    "    df_values.loc[NBumi_ret.index, 'nbumi'] = NBumi_ret['q.value'].values\n",
    "    \n",
    "    # Now we will fill df_ranks with an argsort !!!!! M3DROP and NBumi is not [::-1] because they are q-values \n",
    "    for col in [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke']:\n",
    "        df_ranks[col] = df_values[col].values.argsort()[::-1].argsort()\n",
    "    for col in ['m3drop', 'nbumi']:\n",
    "        df_ranks[col] = df_values[col].values.argsort().argsort() # double argsort to return the rank!\n",
    "    \n",
    "    df_ranks.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_ranks.csv')\n",
    "    df_values.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_values.csv')\n",
    "    \n",
    "    return df_values, df_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random datasets\n",
    "For this section we will use the random datasets generated with splatter.\n",
    "To evaluate the performance of the feature selection methods, we will use teo metrics, maximum deviation and ARI, explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatter_dir = os.path.dirname(os.getcwd()) + '/data/splatter/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS PROCESS TAKES ~ 5 HOURS!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_deprobs = [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]\n",
    "\n",
    "for deprob in tqdm(list_deprobs):\n",
    "    try:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "    except:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    create_df_feature_ranking(adata_deprob, f'scatter_{deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum difference (or maximum variation)\n",
    "We define maximum variation as the maximum value of the differences between groups. The maximum variation is calculated in the following steps:\n",
    "* For each group in the dataset, and for each gene, select the gene if any group has more than X% of expressing cells (25% by default). If all groups have fewer than X% of expressing cells, the variation is set to 0. \n",
    "* If the gene is selected, calculate the trimmed mean (2.5% lowest and highest values removed by default) for each group.\n",
    "* Scale the mean expression array to 1.\n",
    "* Sort the values and select $\\max(|a[0] - a[X]|, |a[-X] - a[-1]|)$ (X = 3 by default). We select first and last values because generally either one or two clusters are overexpressed, or one cluster is undeexpressed, and the expression values will appear at the beginning or end of the mean expression array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the largest difference in all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deprob in tqdm(list_deprobs):\n",
    "    try:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "    except:\n",
    "        adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    \n",
    "    adata_deprob.X = np.asarray(adata_deprob.X.todense())\n",
    "    \n",
    "    groups = sorted(list(dict.fromkeys(adata_deprob.obs['Group'].values)))\n",
    "    df_max_var = pd.DataFrame(index=adata_deprob.var_names, columns=groups + ['maximum_variation'])\n",
    "    \n",
    "    for gene in tqdm(adata_deprob.var_names):\n",
    "        max_var, arr_info = get_max_diff_gene(adata_deprob, gene, 'Group')\n",
    "        df_max_var.loc[gene, groups] = arr_info\n",
    "        df_max_var.loc[gene, 'maximum_variation'] = max_var\n",
    "        \n",
    "    df_max_var.to_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_maximum_variation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot two modalities:\n",
    "* X axis represents the different feature selction methods, and each plot represents a dataset. We will select datasets with \"conflictive\" probabilities, like 0.01 or 0.016. For each feature selection methods, different features from 0-50, 50-100, 100-200, etc. are selected to show the stratification based on rank.\n",
    "* X axis represents different datasets and, for each datasets, different feature selection methods are plotted. Each plot represents a number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.016\n",
    "\n",
    "df_max_var = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_maximum_variation.csv', index_col=0)\n",
    "df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_feature_ranks.csv', index_col=0)\n",
    "df_feature_ranks = df_feature_ranks[['triku_0'] + [i for i in df_feature_ranks.columns if 'triku' not in i]].rename(columns = {'triku_0': 'triku'})\n",
    "\n",
    "plot_max_var_x_method(df_feature_ranks, df_max_var, feature_list = [0, 50, 100, 200, 500, 1000], title=f'Maximum difference by number of features, random w/ DE {deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, at least for triku, differences are more apparent for the first features (50 - 100), and for larger numbers of features the difference distribution diminishes. For features in range 0 to 100 or 200 triku shows the highest values, followed by scanpy and scry. Interestingly, brennecke fails to show any proper feature up to the first 200 features, which is bizarre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_feature_ranks, dict_df_max_var_dataset = {}, {}\n",
    "\n",
    "for deprob in [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]:\n",
    "    df_max_var = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_maximum_variation.csv', index_col=0)\n",
    "    df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_feature_ranks.csv', index_col=0)\n",
    "    df_feature_ranks = df_feature_ranks[['triku_0'] + [i for i in df_feature_ranks.columns if 'triku' not in i]].rename(columns = {'triku_0': 'triku'})\n",
    "    \n",
    "    dict_df_feature_ranks[deprob], dict_df_max_var_dataset[deprob] = df_feature_ranks, df_max_var\n",
    "    \n",
    "\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=100, title='Maximum difference in datasets, 100 features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=200, title='Maximum difference in datasets, 200 features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=500, title='Maximum difference in datasets, 500 features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=2500, title='Maximum difference in datasets, 2500 features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, generally, triku shows the greatest difference across datasets, regardless of the number of features selected. For higher numbers of features the difference becomes less aparent at lower DE ranges(up to 0.025), simply because in those datasets the are not that many distinguishing features and, therefore, if more features are selected, they only contribute as *background noise*. \n",
    "\n",
    "To see why triku does perform better, we are going to choose a moderately difficult dataset (DE = 0.016), plot UMAPs for each method and its first features, and see any patterns that are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.016\n",
    "try:\n",
    "    adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "except:\n",
    "    adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    \n",
    "adata_deprob.X = np.asarray(adata_deprob.X.todense())\n",
    "    \n",
    "tk.tl.triku(adata_deprob)\n",
    "sc.pp.log1p(adata_deprob)\n",
    "sc.pp.pca(adata_deprob, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_deprob, n_neighbors=int(0.5 * len(adata_deprob) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_deprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rows = 60\n",
    "fig, axs = plt.subplots(n_rows, 7, figsize=(7 * 3, n_rows * 2.5))\n",
    "\n",
    "rank_list = list(np.arange(15)) + list(np.linspace(15, 150, n_rows-15).astype(int))\n",
    "\n",
    "for col in range(7):   \n",
    "    for row in range(n_rows):\n",
    "        method = df_feature_ranks.columns[col]\n",
    "        gene = df_feature_ranks[method][df_feature_ranks[method] == rank_list[row]].index[0]\n",
    "        max_var = df_max_var.loc[gene, 'maximum_variation']\n",
    "        \n",
    "        sc.pl.umap(adata_deprob, color=gene, cmap=magma, ax=axs[row][col], title='', show=False)\n",
    "        \n",
    "        axs[row][col].set_xlabel(f\"{gene} ({rank_list[row]}), {max_var:.2f}\")\n",
    "        axs[row][col].set_ylabel('')\n",
    "        axs[row][col].xaxis.set_label_position('top') \n",
    "        axs[row][col].axes.get_xaxis().set_ticks([])\n",
    "        axs[row][col].axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    \n",
    "for col in range(7):\n",
    "    axs[0][col].set_title(df_feature_ranks.columns[col])\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a clear reason why triku selects clusters with more differences. Generally, triku selects genes with appear underexpressed in one cluster, and does not favour as much the selection of genes that are expressed equally among clusters but unequally within each cluster. Therefore, by selecting genes overexpressed or underexpressed in one cluster, its scores are better; but this does not mean that other methods do not do also that. Hoever, judging by the difference scores, it seems that the rest of methods prefer genes that are overexpressed within each cluster rather than overexpressed in one cluster. Again, this is a trend, all methods select genes with overexpression in one cluster (at least the more apparent ones).\n",
    "\n",
    "Brennecke and M3Drop completely fail here because the distributions that are fitted for the methods do not correspond to the ones belonging to the gene count matrix. This does not mean that they won't perform as badly in real datasets, but shows how distribution-fitting dependent methods are unstable in other types of datasets. We will evaluate this performance in the biological benchmarking datasets, to see if they underperform in other-than-usual library preparation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI - Random datasets\n",
    "Using ARI on random datasets is a measure to assess the effectiveness of the feature selection. Random datasets were prepared with different degrees of differentially expressed gene probability, so that we can compare the leiden clusterign solution with the 9 populations. Triku can be run with different seeds, but the rest of methods are deterministic. However, leiden clustering in all cases can be run with a seed. Therefore, we are going to run all processes with 10 seeds (although the deterministic processes will be run once).\n",
    "\n",
    "To apply the ARI we need to run leiden with as many clusters as scatter populations. Since leiden runs on resolution, we need to adjust the resolution parameter to match the number of clusters. To do that we are going to implement a binary search-like algorithm. We will start with resolutions 0.3 and 2. If any of those yields the clusters, done. Else, find the midpoint, run the clustering, and if the clustering yields the number of populations, stop. Else, set the upper or lower resolution to the one that makes the desired number of clusters to be in the middle. This algorithm will try at most 5 times (it gets to resolution differences of ~0.05, which is fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a dictionary where each element is a dataframe of ARIs run with each DE probability. The \n",
    "adata is of dims n_seeds x n_methods. Therefore, cell i,j will have the ARI between the populations and the optimal leiden clustering.\n",
    "\n",
    "We are going to run the calculations with different feature numbers. To save time, we are going to use a high feature number (5000), store the selected features in places, and for future callings, use the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_deprob, dict_features = {}, {}\n",
    "n_seeds, n_feats = 10, 200\n",
    "min_res, max_res, max_depth = 0.1, 2, 6\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.01\n",
    "\n",
    "adata = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', cache=True)\n",
    "sc.pp.filter_genes(adata, min_cells=1)\n",
    "sc.pp.filter_cells(adata, min_genes=1)\n",
    "sc.pp.subsample(adata, fraction=0.55)\n",
    "adata.X = np.asarray(adata.X.todense())\n",
    "adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "adata.obs['groupn'] = adata_groups\n",
    "adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_df = pd.DataFrame(adata.X.T.astype(int), index=adata.var_names, columns=adata.obs_names)\n",
    "%Rpush adata_df n_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NBumi_feats = %R run_NBumi(adata_df, n_feats)\n",
    "scanpy_feats = run_scanpy(adata, n_feats)\n",
    "triku_feats, adata_triku = run_triku(adata, n_feats, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [triku_feats, scanpy_feats, NBumi_feats]:\n",
    "    adata_plot = adata.copy()\n",
    "    sc.pp.log1p(adata_plot)\n",
    "    adata_plot.var['highly_variable'] = [i in f for i in adata_plot.var_names]\n",
    "    sc.pp.pca(adata_plot, use_highly_variable=True)\n",
    "    sc.pp.neighbors(adata_plot, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "\n",
    "    \n",
    "    c_f, res = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), f)\n",
    "    print(f'{res} || ARI :',  ARS(c_f, adata_groups))\n",
    "    \n",
    "    adata_plot.obs['leiden'], adata_plot.obs['groups'] = c_f, adata_groups\n",
    "    sc.tl.umap(adata_plot)\n",
    "    sc.pl.umap(adata_plot, color=['leiden', 'groups'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [triku_feats, scanpy_feats, NBumi_feats]:\n",
    "    adata_plot = adata.copy()\n",
    "    adata_plot.var['highly_variable'] = [i in f for i in adata_plot.var_names]\n",
    "    sc.pp.pca(adata_plot, use_highly_variable=True)\n",
    "    sc.pp.neighbors(adata_plot, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "\n",
    "    \n",
    "    c_f, res = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), f, apply_log=False)\n",
    "    print(f'{res} || ARI :',  ARS(c_f, adata_groups))\n",
    "    \n",
    "    adata_plot.obs['leiden'], adata_plot.obs['groups'] = c_f, adata_groups\n",
    "    sc.tl.umap(adata_plot)\n",
    "    sc.pl.umap(adata_plot, color=['leiden', 'groups'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scanpy = adata.copy()\n",
    "adata_scanpy.var['highly_variable'] = [i in scanpy_feats for i in adata_plot.var_names]\n",
    "sc.pp.pca(adata_scanpy, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_scanpy, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_scanpy)\n",
    "\n",
    "adata_triku = adata.copy()\n",
    "adata_triku.var['highly_variable'] = [i in triku_feats for i in adata_plot.var_names]\n",
    "tk.tl.triku(adata_triku, n_features=n_features, n_windows=100, verbose='error')\n",
    "sc.pp.pca(adata_triku, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_triku, n_neighbors=int(0.5 * len(adata_plot) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_triku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_genes = list(set(triku_feats) & set(scanpy_feats))\n",
    "triku_only =list(set(triku_feats) - set(scanpy_feats))\n",
    "scanpy_only = list(set(scanpy_feats) - set(triku_feats))\n",
    "print(len(common_genes), len(triku_only), len(scanpy_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scanpy, color=['groupn'] + scanpy_only, legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scanpy, color=['groupn'] + triku_only, legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = []\n",
    "labels = []\n",
    "alpha = [0.8 if i in triku_feats or i in scanpy_feats else 0.4 for i in adata.var_names]\n",
    "\n",
    "for i in adata.var_names:\n",
    "    if i in triku_feats and i in scanpy_feats:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif i in triku_feats:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Triku')\n",
    "    elif i in scanpy_feats:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Scanpy')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh = pd.DataFrame({\n",
    "    'm': np.log10(adata_triku.X.mean(0)),\n",
    "    'z': (adata_triku.X == 0).sum(0) / adata_triku.shape[0],\n",
    "    'n': adata_triku.var_names.values,\n",
    "    'd': adata_triku.var[\"emd_distance\"],\n",
    "    'color': color, 'label':labels, 'alpha':alpha\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@d')])\n",
    "\n",
    "p.scatter('m', 'd', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_gene(adata, gene, group_col):\n",
    "    mean_exp_val = []\n",
    "    \n",
    "    groups = sorted(list(set(adata.obs[group_col].values)))\n",
    "    dict_argwhere = {group: np.argwhere(adata.obs[group_col].values == group) for group in groups}\n",
    "    if spr.issparse(adata.X):\n",
    "        exp_gene = np.asarray(adata[:,gene].X.todense()).ravel()\n",
    "    else:\n",
    "        exp_gene = adata[:,gene].X.ravel()\n",
    "    \n",
    "    for g in groups:\n",
    "        exp_group = np.sort(exp_gene[dict_argwhere[g]].ravel())\n",
    "        mean_exp_val.append(np.mean(exp_group[: int(0.90 * len(exp_group))])) # for genes with small expression it may amplify noise\n",
    "        \n",
    "    \n",
    "    mean_exp_val_1 = np.array(mean_exp_val)/sum(mean_exp_val)\n",
    "    \n",
    "#     info = sts.entropy(mean_exp_val_1) / np.log(len(groups))\n",
    "    info = max(np.sort(mean_exp_val_1)[3] - np.sort(mean_exp_val_1)[0], np.sort(mean_exp_val_1)[-1] - np.sort(mean_exp_val_1)[-4])\n",
    "    \n",
    "    return info, mean_exp_val_1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'Gene13823'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_deprob.X = np.asarray(adata_deprob.X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_triku, color=['groupn', gene], legend_loc='on data', cmap=magma)\n",
    "sc.pl.umap(adata_scanpy, color=['groupn', gene], legend_loc='on data', cmap=magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_deprob, dict_features = {}, {}\n",
    "n_seeds, n_feats = 10, 5000\n",
    "min_res, max_res, max_depth = 0.1, 2, 6\n",
    "\n",
    "for deprob in [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]:\n",
    "    adata = sc.read_loom(splatter_dir + f'/splatter_deprob_{deprob}.loom')\n",
    "    adata.X = np.asarray(adata.X.todense())\n",
    "    adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "    adata_df = pd.DataFrame(np.asarray(adata.X.T.todense()), index=adata.var_names, columns=adata.obs_names)\n",
    "    \n",
    "    %Rpush adata_df n_feats\n",
    "    \n",
    "    df = pd.DataFrame(index = np.arange(n_seeds), columns=['triku', 'var', 'scanpy', 'scry', 'brennecke', 'M3Drop', 'NBUmi'])\n",
    "    \n",
    "    for seed in np.arange(n_seeds):\n",
    "        if seed == 0:\n",
    "            triku_feats = run_triku(adata, n_feats, seed)\n",
    "            var_feats = run_variable(adata, n_feats)\n",
    "            scanpy_feats = run_scanpy(adata, n_feats)\n",
    "            NBumi_feats = %R run_NBumi(adata_df, n_feats)\n",
    "            scry_feats = %R run_scry(adata_df, n_feats)\n",
    "            brennecke_feats = %R run_brennecke(adata_df, n_feats)\n",
    "            M3Drop_feats = %R run_M3Drop(adata_df, n_feats)\n",
    "            \n",
    "            dict_features[f'triku_{seed}'], dict_features['var'], dict_features['scanpy'] = triku_feats, var_feats, scanpy_feats\n",
    "            dict_features['scry'], dict_features['brennecke'], dict_features['M3Drop'], dict_features['NBUmi'] = scry_feats, brennecke_feats, M3Drop_feats, NBumi_feats\n",
    "        \n",
    "        else:\n",
    "            triku_feats = run_triku(adata, n_feats, seed)\n",
    "            dict_features[f'triku_{seed}'] = triku_feats\n",
    "            \n",
    "        # Run clustering with each method and get ARI\n",
    "        c_triku = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), triku_feats)\n",
    "        df.loc[seed, 'triku'] = ARI(c_triku, adata_groups)\n",
    "        c_var = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), var_feats)\n",
    "        df.loc[seed, 'var'] = ARI(c_var, adata_groups)\n",
    "        c_scanpy = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scanpy_feats)\n",
    "        df.loc[seed, 'scanpy'] = ARI(c_scanpy, adata_groups)\n",
    "\n",
    "        c_scry = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scry_feats)\n",
    "        df.loc[seed, 'scry'] = ARI(c_scry, adata_groups)\n",
    "        c_brennecke = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), brennecke_feats)\n",
    "        df.loc[seed, 'brennecke'] = ARI(c_brennecke, adata_groups)\n",
    "        c_M3Drop = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), M3Drop_feats)\n",
    "        df.loc[seed, 'M3Drop'] = ARI(c_M3Drop, adata_groups)\n",
    "        c_NBumi = clustering_binary_search(adata, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), scry_NBumi)\n",
    "        df.loc[seed, 'NBumi'] = ARI(c_NBumi, adata_groups)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "aaa <- function(a, b, c){\n",
    "    return(a+b+c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 9\n",
    "b = 19\n",
    "c = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i a -i b -i c -o d\n",
    "\n",
    "d = aaa(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush a b c\n",
    "d = %R aaa(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i n -o a\n",
    "\n",
    "a = run_scry(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush df n\n",
    "a = %Rget run_scry(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_loom(splatter_dir + f'/splatter_deprob_{0.01}.loom')\n",
    "adata.obs['groups'] = [i.replace('Group', '') for i in adata.obs['Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.datasets.pbmc3k()\n",
    "sc.pp.filter_genes(adata, min_cells=10)\n",
    "df = pd.DataFrame(np.asarray(adata.X.T.todense()), index=adata.var_names, columns=adata.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i df -o run_M3Drop\n",
    "\n",
    "run_M3Drop <- run_NBumi(df, 1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df -i adata -o scry_ret -o brennecke_ret -o M3Drop_ret -o NBumi_ret\n",
    "\n",
    "scry_ret <- run_scry(adata, 1500)\n",
    "brennecke_ret <- run_brennecke(adata, 1500)\n",
    "M3Drop_ret <- run_M3Drop(df, 1500)\n",
    "NBumi_ret <- run_NBumi(df, 1500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_M3Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "# Selection of palettes for cluster coloring, and scatter values\n",
    "from palettes_and_cmaps import magma, bold_and_vivid\n",
    "from robustness_functions import run_batch, random_noise_parameter, plot_scatter_parameter, compare_parameter, plot_scatter_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(os.getcwd()) + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/'\n",
    "read_dir = data_dir + 'Ding_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to read you must have: EXACTLY in that manner:\n",
    "# matrix.mtx.gz (you MUST rename it)\n",
    "# features.tsv(you should rename it)\n",
    "# barcodes.tsv (you should rename it)\n",
    "from scipy.io import mmread\n",
    "\n",
    "matrix = mmread('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/matrix.mtx.gz')\n",
    "features = np.loadtxt('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/features.tsv', dtype=str)\n",
    "barcodes = np.loadtxt('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/barcodes.tsv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=matrix.tocsr()).transpose()\n",
    "adata.var_names = features\n",
    "adata.obs_names = barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/media/seth/SETH_DATA/SETH_Alex/triku/data/Ding_2020/preprocessed/mouse/meta_combined.txt', sep='\\t', skiprows=[1])\n",
    "adata = adata[meta['NAME'].values]\n",
    "adata.obs['method'] = meta['Method'].values\n",
    "adata.obs['CellType'] = meta['CellType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1_000\n",
    "a-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_text(read_dir + '/GSE133545_SMARTseq2_human_exp_mat.tsv').transpose()\n",
    "adata.var_names_make_unique()\n",
    "sc.pp.filter_genes(adata, min_cells=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tl.triku(adata, verbose='triku', n_procs=4, knn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_a(a, b): return np.convolve(a, b, )\n",
    "def func_b(a, b): \n",
    "    x = fftconvolve(a, b, )\n",
    "    x[x < 0] = 0\n",
    "    \n",
    "    return x\n",
    "\n",
    "def apply_convolution_read_counts(probs: np.ndarray, knn: int, func) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Convolution of functions. The function applies a convolution using np.convolve\n",
    "    of a probability distribution knn times. The result is an array of N elements (N arises as the convolution\n",
    "    of a n-length array knn times) where the element i has the probability of i being observed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs : np.array\n",
    "        Object with count matrix. If `pandas.DataFrame`, rows are cells and columns are genes.\n",
    "    knn : int\n",
    "        Number of kNN\n",
    "    \"\"\"\n",
    "      \n",
    "    \n",
    "    # We are calculating the convolution of cells with positive expression. Thus, in the first distribution\n",
    "    # we have to remove the cells with 0 reads, and rescale the probabilities.\n",
    "    arr_0 = probs.copy()\n",
    "    arr_0[0] = 0  # TODO: this will fail in log-transformed data\n",
    "    arr_0 /= arr_0.sum()\n",
    "\n",
    "    # We will use arr_bvase as the array with the read distribution\n",
    "    arr_base = probs.copy()\n",
    "    \n",
    "    arr_convolve = func(arr_0, arr_base, )\n",
    "    \n",
    "    for knni in range(2, knn):\n",
    "        arr_convolve = func(arr_convolve, arr_base, )\n",
    "\n",
    "    # TODO: check the probability sum is 1 and, if so, remove\n",
    "    arr_prob = arr_convolve / arr_convolve.sum()\n",
    "\n",
    "    # TODO: if log transformed, this is untrue. Should not be arange.\n",
    "    return np.arange(len(arr_prob)), arr_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_gene = adata.X[:, 377]\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_a, sums_a, times_b, sums_b = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(500, 1000)):\n",
    "    counts_gene = adata.X[:, i]\n",
    "    y_probs = np.bincount(counts_gene.astype(int)) / len(counts_gene)\n",
    "    t = time.time()\n",
    "    apply_convolution_read_counts(y_probs, 50, func_a)\n",
    "    times_a.append(time.time() - t)\n",
    "    sums_a.append(counts_gene.sum())\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    apply_convolution_read_counts(y_probs, 50, func_b)\n",
    "    times_b.append(time.time() - t)\n",
    "    sums_b.append(counts_gene.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.scatter(np.log10(sums_a), np.log10(times_a))\n",
    "plt.scatter(np.log10(sums_b), np.log10(times_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs, len(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = apply_convolution_read_counts(y_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = apply_convolution_read_counts(y_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(((a[1]-b[1])**2)**0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alex-base] *",
   "language": "python",
   "name": "conda-env-alex-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
