{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing other feature selection methods with triku\n",
    "In this notebook we will compare the performance of triku, compared to other methods. \n",
    "\n",
    "The methods that will be compared will be the following:\n",
    "* Select genes with highest variance.   \n",
    "* Scanpy's `sc.pp.highly_variable_genes`: It is based on Seurat's `vst` method, so they should return similar results.\n",
    "* scry `devianceFeatureSelection()`. This method is featured as the feature selection for Irizarry's GLM-PCA paper (https://doi.org/10.1186/s13059-019-1861-6). From its description, it computes a deviance statistic for each row feature for count data based on a multinomial null model that assumes each feature has a constant rate. Features with large deviance are likely to be informative. Uninformative, low deviance features can be discarded to speed up downstream analyses and reduce memory footprint. The `fam`parameter will be set to `binomial`, the default.\n",
    "* M3Drop, which has two main functions:\n",
    "    * NBDrop: the NBDrop model assumes proportion of zeros follows a Michaelis-Menten model. Then the Michaelis-Menten parameter $K$ is fitted. For each gene, its parameter $K_i$ is compared to $K$ using a $Z$-test, which returns the selected genes.\n",
    "    * NBUmi: The procedure is similar to above, although the equation to fit now is a negative binomial model,  and the selection of genes is then done using a $Z$-test.\n",
    "* `BrenneckeGetVariableGenes` fits a function between CV$^2$ and mean expression. \n",
    "\n",
    "With the exception of scanpy and triku, the rest of functions are set on $R$. We will use jupyter's `%%R` magic command, and `anndata2ri` to transform `annData` into `SingleCellExperiment` objects, and we will generate the functions to accept that annData and return the list of selected features. The functions have to be set up in notebook, and cannot be externalized. \n",
    "\n",
    "M3Drop requires a normalization step, which will be done in-situ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triku as tk\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as spr\n",
    "import scipy.stats as sts\n",
    "import os\n",
    "import gc\n",
    "from itertools import product\n",
    "import pickle\n",
    "import ray\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bokeh.io import show, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearColorMapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ARS\n",
    "from sklearn.metrics import adjusted_mutual_info_score as NMI\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "from triku_nb_code.comparing_feat_sel import plot_max_var_x_dataset, plot_max_var_x_method, create_dict_UMAPs_datasets, \\\n",
    "get_max_diff_gene, plot_ARI_x_method, plot_ARI_x_dataset, biological_silhouette_ARI_table, plot_lab_org_comparison_scores, \\\n",
    "clustering_binary_search, compare_rankings, compare_values\n",
    "from triku_nb_code.comparing_feat_sel import create_UMAP_adataset_libprep_org, plot_UMAPs_datasets, plot_XY, biological_silhouette_ARI_table\n",
    "from triku_nb_code.palettes_and_cmaps import magma, bold_and_vivid, prism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.rinterface_lib.callbacks, logging\n",
    "from rpy2.robjects import pandas2ri\n",
    "import anndata2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore R warning messages\n",
    "#Note: this can be commented out to get more verbose R output\n",
    "rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Automatically convert rpy2 outputs to pandas dataframes\n",
    "anndata2ri.activate()\n",
    "pandas2ri.activate()\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext rmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load all the R libraries we will be using in the notebook\n",
    "library(M3Drop) # Depends on r-foreing (conda-forge) and Hmisc and reldist (install.packages)\n",
    "library(scry) # If R < 4, launch commit 9f0fc819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + '/exports/comparisons/', exist_ok=True)\n",
    "os.makedirs(os.getcwd() + '/figures/comparison_figs/png', exist_ok=True)\n",
    "os.makedirs(os.getcwd() + '/figures/comparison_figs/pdf', exist_ok=True)\n",
    "data_dir = os.path.dirname(os.getcwd()) + '/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following 3 cells we will create the cells that obtain the most relevant features. Since some of the calls are to R, they have to be kept as separate cells. Also, we create the function `create_df_feature_ranking` which creates two dataframes: one with the evaluation values (p-value, emd distance, etc.) of each method, and the second one with the ranking of genes based on those values. These dataframes will be valuable so that we don't have to repeat the calling to the feature selection methods each time we do a graph. `create_df_feature_ranking` is also kept as a cell because it makes some calls to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "run_scry <- function(sce){ #adata\n",
    "    adata_ret = devianceFeatureSelection(sce, nkeep=dim(sce)[1])\n",
    "    return(adata_ret) #returns adata with stats on .var\n",
    "} \n",
    "\n",
    "\n",
    "run_brennecke <- function(sce){ #df\n",
    "    res_df <- BrenneckeGetVariableGenes(sce, suppress.plot=TRUE, fdr=100)\n",
    "    return(res_df) # returns sorted df with genes and stats\n",
    "}\n",
    "\n",
    "\n",
    "run_M3Drop <- function(sce){\n",
    "    norm <- M3DropConvertData(sce, is.counts=TRUE)\n",
    "    DE_genes <- M3DropFeatureSelection(norm, suppress.plot=TRUE, mt_threshold=50)\n",
    "    return(DE_genes) # returns sorted df with genes and stats\n",
    "    \n",
    "}\n",
    "\n",
    "run_NBumi <- function(sce){\n",
    "    count_mat <- NBumiConvertData(sce, is.counts=TRUE)\n",
    "    DANB_fit <- NBumiFitModel(count_mat)\n",
    "    NBDropFS <- NBumiFeatureSelectionCombinedDrop(DANB_fit, suppress.plot=TRUE, qval.thresh=10)\n",
    "    return(NBDropFS)  # returns sorted df with genes and stats\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scanpy(adata):\n",
    "    adata_copy = adata.copy()\n",
    "    if not 'log1p' in adata_copy.uns:\n",
    "        sc.pp.log1p(adata_copy)\n",
    "    ret = sc.pp.highly_variable_genes(adata_copy, n_top_genes=len(adata_copy), inplace=False)\n",
    "    df = pd.DataFrame(ret)\n",
    "    df =  df.set_index(adata_copy.var_names)\n",
    "    del adata_copy; gc.collect()\n",
    "    return df # returns df with stats\n",
    "\n",
    "def run_variable(adata):\n",
    "    if spr.issparse(adata.X):\n",
    "        std = adata.X.power(2).mean(0) - np.power(adata.X.mean(0), 2) \n",
    "        std = np.asarray(std).flatten()        \n",
    "    else:\n",
    "        std = adata.X.std(0)\n",
    "        \n",
    "    return std #returns vector with order as var_names \n",
    "\n",
    "def run_triku(adata, seed):\n",
    "    adata_copy = adata.copy()\n",
    "    tk.tl.triku(adata_copy, n_comps=30, n_windows=100, random_state=seed, verbose='error')\n",
    "    d = adata_copy.var['emd_distance'] #pd series with distance\n",
    "    del adata_copy; gc.collect()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_feature_ranking(adatax, title_prefix, apply_log=False):\n",
    "    \"\"\"\n",
    "    Create a dataframe with the ranking of features, and another one with the feature values. The adata must be the raw\n",
    "    adata. From that we will create a adata_df necessary for some R methods.\n",
    "    The adata will include:\n",
    "    - Triku with 10 seeds 'triku_SEEDN'\n",
    "    - Scanpy's HVG 'scanpy'\n",
    "    - Std 'std'\n",
    "    - scry 'scry'\n",
    "    - brennecke 'brennecke'\n",
    "    - M3Drop 'm3drop'\n",
    "    - NBumi 'nbumi'\n",
    "    \n",
    "    After each method is run, we will fill the dataframe values, with the values of the metrics used for feature selection, \n",
    "    and the dataframe of rankings with the rankings based on the returned value (0, 1, 2, etc.). \n",
    "    We create two separate dataframes because the df with values might be reserved for other purposes. The rank dataframes is interesting\n",
    "    because the values on the values dataframe have different argsort orders depending on the column (M3drop and NBumi direct, rest reverse).\n",
    "    \"\"\"\n",
    "    \n",
    "    adata = adatax.copy()\n",
    "    sc.pp.filter_genes(adata, min_cells=1) \n",
    "    sc.pp.filter_cells(adata, min_genes=1)\n",
    "    \n",
    "    if apply_log:\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    adata_df = pd.DataFrame(adata.X.T, index=adata.var_names, columns=adata.obs_names)\n",
    "        \n",
    "    adata_short = sc.AnnData(X = adata.X[:,:]) # we have to create a clean adata because some column break Rpush\n",
    "    adata_short.var_names, adata_short.obs_names = adata.var_names[:], adata.obs_names[:]\n",
    "    print(adata_short.shape)\n",
    "    \n",
    "    %Rpush adata_short\n",
    "    %Rpush adata_df\n",
    "    \n",
    "    print('Outside R', adata.shape, adata_short.shape)\n",
    "    d = %R  dim(adata_short)\n",
    "    print('Inside R', d)\n",
    "       \n",
    "    if 'Group' in adata.obs:\n",
    "        adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "        adata.obs['groupn'] = adata_groups\n",
    "    \n",
    "    index, columns = adata.var_names, [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi']\n",
    "    df_values, df_ranks = pd.DataFrame(index=index, columns=columns), pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    for i in range(10):\n",
    "        df_emd_distance = run_triku(adata, i)\n",
    "        df_values.loc[df_emd_distance.index, f'triku_{i}'] = df_emd_distance.values\n",
    "        \n",
    "    \n",
    "    scanpy_ret = run_scanpy(adata)\n",
    "    df_values.loc[scanpy_ret.index, 'scanpy'] = scanpy_ret['dispersions_norm'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    std_ret = run_variable(adata)\n",
    "    df_values.loc[:, 'std'] = std_ret\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    scry_ret = %R run_scry(adata_short)\n",
    "    df_values.loc[scry_ret.var.index, 'scry'] = scry_ret.var['binomial_deviance'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    brennecke_ret = %R run_brennecke(adata_df)\n",
    "    df_values.loc[brennecke_ret.index, 'brennecke'] = brennecke_ret['effect.size'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    M3Drop_ret = %R run_M3Drop(adata_df)\n",
    "    df_values.loc[M3Drop_ret.index, 'm3drop'] = M3Drop_ret['q.value'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    NBumi_ret = %R run_NBumi(adata_df)\n",
    "    df_values.loc[NBumi_ret.index, 'nbumi'] = NBumi_ret['q.value'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    # Now we will fill df_ranks with an argsort !!!!! M3DROP and NBumi is not [::-1] because they are q-values \n",
    "    for col in [f'triku_{i}' for i in range(10)] + ['scanpy', 'std', 'scry', 'brennecke']:\n",
    "        df_ranks[col] = df_values[col].values.argsort()[::-1].argsort()\n",
    "    for col in ['m3drop', 'nbumi']:\n",
    "        df_ranks[col] = df_values[col].values.argsort().argsort() # double argsort to return the rank!\n",
    "    \n",
    "    df_ranks.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_ranks.csv')\n",
    "    df_values.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_values.csv')\n",
    "    print('df_ranks', df_ranks.shape)\n",
    "    \n",
    "    del adata; gc.collect()\n",
    "    return df_values, df_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random datasets\n",
    "For this section we will use the random datasets generated with splatter.\n",
    "To evaluate the performance of the feature selection methods, we will use teo metrics, maximum deviation and ARI, explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatter_dir = os.path.dirname(os.getcwd()) + '/data/splatter/'\n",
    "list_deprobs = [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS PROCESS TAKES ~ 5 HOURS!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO Parallelize?\n",
    "for deprob in tqdm(list_deprobs):\n",
    "    print(f'NOW DEPROB IS {deprob}!!!!!!')\n",
    "    adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', sparse=False)\n",
    "        \n",
    "    create_df_feature_ranking(adata_deprob, f'scatter_{deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum difference (or maximum variation)\n",
    "We define maximum variation as the maximum value of the differences between groups. The maximum variation is calculated in the following steps:\n",
    "* For each group in the dataset, and for each gene, select the gene if any group has more than X% of expressing cells (25% by default). If all groups have fewer than X% of expressing cells, the variation is set to 0. \n",
    "* If the gene is selected, calculate the trimmed mean (2.5% lowest and highest values removed by default) for each group.\n",
    "* Scale the mean expression array to 1.\n",
    "* Sort the values and select $\\max(|a[0] - a[X]|, |a[-X] - a[-1]|)$ (X = 3 by default). We select first and last values because generally either one or two clusters are overexpressed, or one cluster is undeexpressed, and the expression values will appear at the beginning or end of the mean expression array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the largest difference in all dataset\n",
    "for deprob in tqdm(list_deprobs):\n",
    "    adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', sparse=False)\n",
    "        \n",
    "    sc.pp.filter_genes(adata_deprob, min_cells=1)\n",
    "    sc.pp.filter_cells(adata_deprob, min_genes=1)\n",
    "    print(adata_deprob.shape)\n",
    "    groups = sorted(list(dict.fromkeys(adata_deprob.obs['Group'].values)))\n",
    "    df_max_var = pd.DataFrame(index=adata_deprob.var_names, columns=groups + ['maximum_variation'])\n",
    "    \n",
    "    for gene in tqdm(adata_deprob.var_names):\n",
    "        max_var, arr_info = get_max_diff_gene(adata_deprob, gene, 'Group')\n",
    "        df_max_var.loc[gene, groups] = arr_info\n",
    "        df_max_var.loc[gene, 'maximum_variation'] = max_var\n",
    "    \n",
    "    print(df_max_var.shape)\n",
    "\n",
    "    df_max_var.to_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_maximum_variation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot two modalities:\n",
    "* X axis represents the different feature selction methods, and each plot represents a dataset. We will select datasets with \"conflictive\" probabilities, like 0.01 or 0.016. For each feature selection methods, different features from 0-50, 50-100, 100-200, etc. are selected to show the stratification based on rank.\n",
    "* X axis represents different datasets and, for each datasets, different feature selection methods are plotted. Each plot represents a number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.016\n",
    "\n",
    "df_max_var = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_maximum_variation.csv', index_col=0)\n",
    "df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprob}' + '_feature_ranks.csv', index_col=0)\n",
    "df_feature_ranks = df_feature_ranks[['triku_0'] + [i for i in df_feature_ranks.columns if 'triku' not in i]].rename(columns = {'triku_0': 'triku'})\n",
    "\n",
    "plot_max_var_x_method(df_feature_ranks, df_max_var, feature_list = [0, 50, 100, 200, 500, 1000], \n",
    "                      title=f'Maximum difference by number of features, random w/ DE {deprob}', \n",
    "                      file=f'max_diff_scatter_DE-{deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, at least for triku, differences are more apparent for the first features (50 - 100), and for larger numbers of features the difference distribution diminishes. For features in range 0 to 100 or 200 triku shows the highest values, followed by scanpy and scry. Interestingly, brennecke fails to show any proper feature up to the first 200 features, which is bizarre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_feature_ranks, dict_df_max_var_dataset = {}, {}\n",
    "\n",
    "for deprobx in [0.005, 0.0065, 0.008, 0.01, 0.013, 0.016, 0.025, 0.05, 0.1, 0.3]:\n",
    "    df_max_var = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprobx}' + '_maximum_variation.csv', index_col=0)\n",
    "    df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprobx}' + '_feature_ranks.csv', index_col=0)\n",
    "    df_feature_ranks = df_feature_ranks[['triku_0'] + [i for i in df_feature_ranks.columns if 'triku' not in i]].rename(columns = {'triku_0': 'triku'})\n",
    "    \n",
    "    dict_df_feature_ranks[deprobx], dict_df_max_var_dataset[deprobx] = df_feature_ranks, df_max_var\n",
    "    \n",
    "\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=100, title='Maximum difference in datasets, 100 features', \n",
    "                      file=f'max_diff_scatter_all-DE_100-features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=200, title='Maximum difference in datasets, 200 features', \n",
    "                      file=f'max_diff_scatter_all-DE_200-features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=500, title='Maximum difference in datasets, 500 features', \n",
    "                      file=f'max_diff_scatter_all-DE_500-features')\n",
    "plot_max_var_x_dataset(dict_df_feature_ranks, dict_df_max_var_dataset, n_features=2500, title='Maximum difference in datasets, 2500 features', \n",
    "                      file=f'max_diff_scatter_all-DE_2500-features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, generally, triku shows the greatest difference across datasets, regardless of the number of features selected. For higher numbers of features the difference becomes less aparent at lower DE ranges(up to 0.025), simply because in those datasets the are not that many distinguishing features and, therefore, if more features are selected, they only contribute as *background noise*. \n",
    "\n",
    "To see why triku does perform better, we are going to choose a moderately difficult dataset (DE = 0.016), plot UMAPs for each method and its first features, and see any patterns that are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprob = 0.016\n",
    "adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', sparse=False)\n",
    "\n",
    "sc.pp.filter_genes(adata_deprob, min_cells=1)\n",
    "sc.pp.filter_cells(adata_deprob, min_genes=1)\n",
    "    \n",
    "tk.tl.triku(adata_deprob)\n",
    "sc.pp.log1p(adata_deprob)\n",
    "sc.pp.pca(adata_deprob, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_deprob, n_neighbors=int(0.5 * len(adata_deprob) ** 0.5), metric='cosine')\n",
    "sc.tl.umap(adata_deprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rows = 60\n",
    "fig, axs = plt.subplots(n_rows, 7, figsize=(7 * 3, n_rows * 2.5))\n",
    "\n",
    "rank_list = list(np.arange(15)) + list(np.linspace(15, 150, n_rows-15).astype(int))\n",
    "\n",
    "for col, method in enumerate(df_feature_ranks.columns):   \n",
    "    for row in range(n_rows):\n",
    "        gene = df_feature_ranks[method][df_feature_ranks[method] == rank_list[row]].index[0]\n",
    "        max_var = df_max_var.loc[gene, 'maximum_variation']\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            sc.pl.umap(adata_deprob, color=gene, cmap=magma, ax=axs[row][col], title='', show=False)\n",
    "        except: \n",
    "            print(f'Fail in col {col} / row {row}')\n",
    "            pass # sometimes it fails (failed once and dunno why)\n",
    "        \n",
    "        axs[row][col].set_xlabel(f\"{gene} ({rank_list[row]}), {max_var:.2f}\")\n",
    "        axs[row][col].set_ylabel('')\n",
    "        axs[row][col].xaxis.set_label_position('top') \n",
    "        axs[row][col].axes.get_xaxis().set_ticks([])\n",
    "        axs[row][col].axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    \n",
    "for col in range(7):\n",
    "    axs[0][col].set_title(df_feature_ranks.columns[col])\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a clear reason why triku selects clusters with more differences. Generally, triku selects genes with appear underexpressed in one cluster, and does not favour as much the selection of genes that are expressed equally among clusters but unequally within each cluster. Therefore, by selecting genes overexpressed or underexpressed in one cluster, its scores are better; but this does not mean that other methods do not do also that. Hoever, judging by the difference scores, it seems that the rest of methods prefer genes that are overexpressed within each cluster rather than overexpressed in one cluster. Again, this is a trend, all methods select genes with overexpression in one cluster (at least the more apparent ones).\n",
    "\n",
    "Brennecke and M3Drop completely fail here because the distributions that are fitted for the methods do not correspond to the ones belonging to the gene count matrix. This does not mean that they won't perform as badly in real datasets, but shows how distribution-fitting dependent methods are unstable in other types of datasets. We will evaluate this performance in the biological benchmarking datasets, to see if they underperform in other-than-usual library preparation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI / NMI\n",
    "Using ARI on random datasets is a measure to assess the effectiveness of the feature selection. Random datasets were prepared with different degrees of differentially expressed gene probability, so that we can compare the leiden clusterign solution with the 9 populations. Triku can be run with different seeds, but the rest of methods are deterministic. However, leiden clustering in all cases can be run with a seed. Therefore, we are going to run all processes with 10 seeds (although the deterministic processes will be run once).\n",
    "\n",
    "To apply the ARI we need to run leiden with as many clusters as scatter populations. Since leiden runs on resolution, we need to adjust the resolution parameter to match the number of clusters. To do that we are going to implement a binary search-like algorithm. We will start with resolutions 0.3 and 2 (may change in the future). If any of those yields the clusters, done. Else, find the midpoint, run the clustering, and if the clustering yields the number of populations, stop. Else, set the upper or lower resolution to the one that makes the desired number of clusters to be in the middle. This algorithm will try at most 5 times (it gets to resolution differences of ~0.05, which is fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the ARI, we need to load a dataset, select a number of features, and create the dataframe with seeds as rows (to see varation on clustering / triku) and the methods as columns. Because creating each dataframe take time (there are 70 cells to be filled), we will choose two datasets (DE = 0.01 and 0.025) and two number of features (100 and 500), which show good results in the previous sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_res, max_res, max_depth = 0.3, 2, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deprobx in tqdm([0.0065, 0.008, 0.01, 0.016, 0.025, 0.05, 0.1, 0.3]):\n",
    "    adata_all = sc.read(splatter_dir + f'/splatter_deprob_{deprobx}.loom', sparse=False)\n",
    "    sc.pp.subsample(adata_all, 0.4) # We shorthen this to make the calculations not take 8 ours!\n",
    "    sc.pp.filter_genes(adata_all, min_cells=1)\n",
    "    sc.pp.filter_cells(adata_all, min_genes=1)\n",
    "    sc.pp.log1p(adata_all)\n",
    "    \n",
    "    for n_features in tqdm([100, 250, 500]):\n",
    "        print(deprobx, n_features)\n",
    "        if not (os.path.exists(os.getcwd() + f'/exports/comparisons/ARI_scatter_{deprobx}_n_features_{n_features}.csv') & \n",
    "                os.path.exists(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv')):\n",
    "            df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprobx}' + '_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            list_methods = ['triku'] + [i for i in df_feature_ranks.columns if not i.startswith('triku')] + ['all', 'random']\n",
    "\n",
    "            df_ARI = pd.DataFrame(index=[f'seed_{i}' for i in range(10)], columns=list_methods)\n",
    "            df_NMI = pd.DataFrame(index=[f'seed_{i}' for i in range(10)], columns=list_methods)\n",
    "\n",
    "            for seed in tqdm(range(10)):\n",
    "                for method in tqdm(list_methods):\n",
    "                    if method.startswith('triku'):\n",
    "                        feats = df_feature_ranks[f'triku_{seed}'].sort_values().index[:n_features]\n",
    "                    \n",
    "                    elif method == \"all\":\n",
    "                        feats = df_feature_ranks[f'triku_{seed}'].sort_values().index[:]\n",
    "                    elif method == \"random\":\n",
    "                        array_selection = np.array([False] * len(df_feature_ranks))\n",
    "                        array_selection[np.random.choice(np.arange(len(df_feature_ranks)), n_features, replace=False)] = True\n",
    "                        \n",
    "                        feats = df_feature_ranks[f'triku_{seed}'].sort_values().index[array_selection]\n",
    "                    \n",
    "                    else:\n",
    "                        feats = df_feature_ranks[method].sort_values().index[:n_features]\n",
    "                    \n",
    "                    adata_groups = [i.replace('Group', '') for i in adata_all.obs['Group']]\n",
    "                    c_f, res = clustering_binary_search(adata_all, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), feats, apply_log=False)\n",
    "                    ARI = ARS(c_f, adata_groups)\n",
    "                    NMS = NMI(c_f, adata_groups)\n",
    "\n",
    "                    df_ARI.loc[f'seed_{seed}', method] = ARI\n",
    "                    df_NMI.loc[f'seed_{seed}', method] = NMS\n",
    "            \n",
    "            print(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv')\n",
    "            df_ARI.to_csv(os.getcwd() + f'/exports/comparisons/ARI_scatter_{deprobx}_n_features_{n_features}.csv')\n",
    "            df_NMI.to_csv(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_deprobs = [0.0065, 0.008, 0.01, 0.016, 0.025, 0.05, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_feats in ['100', '250', '500']:\n",
    "    list_files = [f'ARI_scatter_{deprob}_n_features_{n_feats}.csv' for deprob in list_deprobs[::-1]]\n",
    "    plot_lab_org_comparison_scores(f'ARI-{n_feats}', '', save_dir, [''], increasing=0, mode='ARI', list_files=list_files, \n",
    "                                       title=f'ARI on artificial datasets, {n_feats} features', \n",
    "                                       filename=f'ARI_{n_feats}-features')\n",
    "\n",
    "for n_feats in ['100', '250', '500']:\n",
    "    list_files = [f'NMI_scatter_{deprob}_n_features_{n_feats}.csv' for deprob in list_deprobs[::-1]]\n",
    "    plot_lab_org_comparison_scores(f'NMI-{n_feats}', '', save_dir, [''], increasing=0, mode='NMI', list_files=list_files, \n",
    "                                       title=f'NMI on artificial datasets, {n_feats} features', \n",
    "                                       filename=f'NMI_{n_feats}-features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lower number of features (100 - 250) scanpy performs best at lower DE probabilities (up to 0.025) but performs worse at full resolution (0.1 or 0.3), with scry the best method for that, principally because the features that make smaller clusters separate are the ones with most expression, and those are the ones selected by scry. However, at smaller DE probabilities, the features that separate the dataset the most are the ones with mid expression levels, which are best picked by triku. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.read_csv(f'{save_dir}/scatter_0.01_feature_values.csv', index_col=0)\n",
    "df_ranks = pd.read_csv(f'{save_dir}/scatter_0.01_feature_ranks.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(splatter_dir + f'/splatter_deprob_0.01.loom', sparse=False)\n",
    "sc.pp.subsample(adata, 0.4) # We shorthen this to make the calculations not take 8 ours!\n",
    "\n",
    "adata.raw = adata\n",
    "sc.pp.pca(adata)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bokeh = pd.DataFrame({'m': np.log10(adata.X.mean(0)), \n",
    "                         'z': df['triku_0'].loc[adata.var_names].values, \n",
    "                         'n': df.index.values})\n",
    "                   \n",
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=600, plot_width=600, tooltips=[(\"Gene\",\"@n\")])\n",
    "p.scatter('m', 'z', source=df_bokeh, alpha=0.7, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triku_hvg = df_ranks['triku_0'].values < 250\n",
    "scry_hvg = df_ranks['scry'].values < 250\n",
    "std_hvg = df_ranks['std'].values < 250\n",
    "scanpy_hvg = df_ranks['scanpy'].values < 250\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "names = ['triku', 'scanpy', 'std', 'scry',]\n",
    "hvgs = [triku_hvg, scanpy_hvg, std_hvg, scry_hvg]\n",
    "colors = ['#e73f74', '#7f3c8d', '#11a579', '#3969ac']\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i].scatter(np.log10(adata.X.mean(0))[~hvgs[i]][::5], \n",
    "                   df['triku_0'].loc[adata.var_names].values[~hvgs[i]][::5], c=\"#dedede\", s=2, alpha=0.7)\n",
    "    axs[i].scatter(np.log10(adata.X.mean(0))[hvgs[i]], \n",
    "                   df['triku_0'].loc[adata.var_names].values[hvgs[i]], c=colors[i], s=2, label=names[i])\n",
    "    axs[i].legend()\n",
    "\n",
    "fig.text(0.0, 0.5, 'Wasserstein distance', va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.0, 'log$_{10}$ mean expression', va='center', rotation='horizontal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.getcwd() + f'/figures/comparison_figs/pdf/barplots_scatter.pdf', fmt='pdf')\n",
    "\n",
    "\n",
    "list_list_genes = [['Gene1118', 'Gene8599', 'Gene1513', 'Gene1479'],     # triku only\n",
    "                   ['Gene6723', 'Gene6625', 'Gene9796', 'Gene935'],      # triku + scanpy\n",
    "                   ['Gene12841', 'Gene10739', 'Gene6729', 'Gene12240'],  # scanpy\n",
    "                   ['Gene9545', 'Gene4459', 'Gene383', 'Gene12455'],     # all\n",
    "                   ['Gene1633', 'Gene10792', 'Gene2496', 'Gene12497']]   # std + scry\n",
    "\n",
    "list_bar_colors = ['#94346E', '#E17C05', '#0F8554', '#1D6996']\n",
    "\n",
    "for lg_idx, list_genes in enumerate(list_list_genes):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(3*5, 3))\n",
    "    \n",
    "    hvg = np.isin(adata.var_names, list_genes)\n",
    "    axs[0].scatter(np.log10(adata.X.mean(0))[~hvg][::5], \n",
    "                   df['triku_0'].loc[adata.var_names].values[~hvg][::5], c=\"#bcbcbc\", s=2, alpha=0.7)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        for group in range(10):\n",
    "            axs[0].scatter(np.log10(adata.X.mean(0))[np.isin(adata.var_names, list_genes[i - 1])][::5], \n",
    "                           df['triku_0'].loc[adata.var_names].values[np.isin(adata.var_names, list_genes[i - 1])][::5], c=list_bar_colors[i - 1], s=7)\n",
    "\n",
    "                \n",
    "            data_values = adata[adata.obs['Group'] == 'Group' + str(group + 1)].X[:, np.argwhere(adata.var_names == list_genes[i-1])[0]].flatten()\n",
    "            mean, std = np.mean(data_values), np.std(data_values)\n",
    "\n",
    "            axs[i].bar(group + 1, mean, color=list_bar_colors[i - 1])\n",
    "            \n",
    "    axs[0].set_ylabel('Wasserstein distance')\n",
    "    axs[0].set_xlabel('log$_{10}$ mean expression')\n",
    "    axs[1].set_ylabel('Mean group expression')\n",
    "    axs[1].set_xlabel('Group')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.getcwd() + f'/figures/comparison_figs/pdf/barplots_{lg_idx}.pdf', fmt='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ding et al. / Mereu et al. datasets\n",
    "Now that we have seen that triku outperforms other methods in artificial datasets, at least when there is intrinsic noisiness, we are going to apply similar metrics to biological datasets. We are first going to use Mereu's and Ding's human + mouse benchmarking datasets. They will help us see biases on performance of all the methods, and also it will act as a validation of the results from the original papers.\n",
    "\n",
    "In this part, due to the large amount of datasets, and also due to the heterogeneity of genes, we will not apply use different number of features. Instead, we will run triku with seed 0, and select the default number of features that is automatically generated to select the features on the rest of methods. This will mean that different datasets will have different number of features, although each dataset will have the same number of features across methods. \n",
    "\n",
    "The two main methods that we will use to evaluate the feature selection are ARI and Silhouette scores.\n",
    "* ARI uses the assigned cell types from the paper (Mereu et al. use MatchSCore2 and Ding et al. uses a custom algorithm) and applies the same binary search for resolution.\n",
    "* Silhouette. It is used in two forms:\n",
    "    * Apply the same resolution to all datasets and all methods using the binary search, and get the Silhouette from there.\n",
    "    * Apply Silhouette to the benchmark-assigned cell types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature ranking dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mereu_dir = os.path.dirname(os.getcwd()) + '/data/Mereu_2020/'\n",
    "\n",
    "for libprep in tqdm(['10X', 'CELseq2', 'ddSEQ', 'Dropseq', 'inDrop', 'QUARTZseq', 'SingleNuclei', 'SMARTseq2']):\n",
    "    for org in ['human', 'mouse']:\n",
    "        if os.path.exists(save_dir + f'mereu_{libprep}_{org}_feature_values.csv'):\n",
    "            print(f'{libprep}, {org} exists!')\n",
    "        else:\n",
    "            adata_libprep = sc.read(mereu_dir + f'{libprep}_{org}.h5ad')\n",
    "            create_df_feature_ranking(adata_libprep, f'mereu_{libprep}_{org}', apply_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ding_dir = os.path.dirname(os.getcwd()) + '/data/Ding_2020/'\n",
    "\n",
    "for libprep in tqdm(['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNA-seq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2']):\n",
    "    for org in ['human', 'mouse']:\n",
    "        if os.path.exists(ding_dir + f'{libprep}_{org}.h5ad'):\n",
    "            if os.path.exists(save_dir + f'ding_{libprep}_{org}_feature_values.csv'):\n",
    "                print(f'{libprep}, {org} exists!')\n",
    "            else:\n",
    "                adata_libprep = sc.read(ding_dir + f'{libprep}_{org}.h5ad')\n",
    "                create_df_feature_ranking(adata_libprep, f'ding_{libprep}_{org}', apply_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_ARI_silhouette_rem(lib_prep, org, seed, lab, adata_dir, save_dir):\n",
    "    if os.path.exists(adata_dir + f'{lib_prep}_{org}.h5ad'):\n",
    "        if os.path.exists(save_dir + f'{lab}_{lib_prep}_{org}_comparison-scores_seed-{seed}.csv'):\n",
    "            print(f'{lib_prep}, {org}, {seed} exists!')\n",
    "        else:\n",
    "            adata = sc.read_h5ad(adata_dir + f'{lib_prep}_{org}.h5ad')\n",
    "            cell_type = 'cell_types' if 'cell_types' in adata.obs else 'CellType' # Somwhere I've fucked up with column name. Don't care where honestly.\n",
    "            df_rank = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{lib_prep}_{org}_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            biological_silhouette_ARI_table(adata, df_rank, outdir=save_dir, file_root=f'{lab}_{lib_prep}_{org}', seed=seed, \n",
    "                                                        cell_types_col=cell_type, n_procs=1)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mereu's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Mereu_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq']\n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'mereu', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ding's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Ding_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNAseq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2']\n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'ding', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lab_org_comparison_scores('mereu', org='-', read_dir=save_dir, variables=['NMI'], figsize=(16, 4), title=f'NMI on mereu datasets', \n",
    "                                  filename=f'mereu-NMI', increasing=False, lognames=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['mereu', 'ding']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    log = 1\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['NMI'], figsize=(16, 4), title=f'NMI on {lab} datasets', \n",
    "                                  filename=f'{lab}-NMI', increasing=False, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['ARI'], figsize=(16, 4), title=f'ARI on {lab} datasets', \n",
    "                                  filename=f'{lab}-ARI', increasing=False, lognames=log)\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on UMAP projection',\n",
    "                                   filename=f'{lab}-silhouette_UMAP_celltypes', increasing=False, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on UMAP projection', \n",
    "                                  filename=f'{lab}-silhouette_UMAP_leiden', increasing=False, lognames=log)\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on PCA projection', \n",
    "                                  filename=f'{lab}-silhouette_PCA_celltypes', increasing=False, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on PCA projection', \n",
    "                                  filename=f'{lab}-silhouette_PCA_leiden', increasing=False, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, cell types on PCA projection [less is better]', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_celltypes', increasing=True, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, leiden clusters on PCA projection [less is better]', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_leiden', increasing=True, lognames=log)\n",
    "    \n",
    "\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on selected features',\n",
    "                                   filename=f'{lab}-silhouette_selected features_celltypes', increasing=False, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on selected features', \n",
    "                                  filename=f'{lab}-silhouette_selected features_leiden', increasing=False, lognames=log)\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_bench_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, cell types on selected features',\n",
    "                                   filename=f'{lab}-Davies-Bouldin_selected features_celltypes', increasing=True, lognames=log)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_leiden_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, leiden clusters on selected features', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_selected features_leiden', increasing=True, lognames=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a clear trend in the results. For all metrics (NMI, ARI, Silhouette and Davies-Bouldin) there are two clear groups regarding performance: on the one hand std + scry + brennecke + sscanpy, where the method rely directly or indirectly in the calculation of a variance metric; and triku + nbumi + m3drop, which do not. The methods relying on a metric perform much worse than those which do not. In some cases scanpy's performance is better, comparable to triku + m3drop + nbumi, but oftentimes the metric appears half both types of methods.\n",
    "\n",
    "Regarding triku's performance, it is at the same level at m3drop and nbumi's. In certain cases, like silhouette scores for 10x human/mouse, Celseq2 mouse and QUARTZseq human, the scores are worse than for nbumi + m3drop, although ARI and NMI have comparabel values. In many other cases, like most of Ding datasets, and Mereu's Dropseq, Smartseq2 and inDrop, Silhouette scores are highest on triku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ding et al. / Mereu et al. datasets (this time with log)\n",
    "Now that we have seen that triku outperforms other methods in artificial datasets, at least when there is intrinsic noisiness, we are going to apply similar metrics to biological datasets. We are first going to use Mereu's and Ding's human + mouse benchmarking datasets. They will help us see biases on performance of all the methods, and also it will act as a validation of the results from the original papers.\n",
    "\n",
    "In this part, due to the large amount of datasets, and also due to the heterogeneity of genes, we will not apply use different number of features. Instead, we will run triku with seed 0, and select the default number of features that is automatically generated to select the features on the rest of methods. This will mean that different datasets will have different number of features, although each dataset will have the same number of features across methods. \n",
    "\n",
    "The two main methods that we will use to evaluate the feature selection are ARI and Silhouette scores.\n",
    "* ARI uses the assigned cell types from the paper (Mereu et al. use MatchSCore2 and Ding et al. uses a custom algorithm) and applies the same binary search for resolution.\n",
    "* Silhouette. It is used in two forms:\n",
    "    * Apply the same resolution to all datasets and all methods using the binary search, and get the Silhouette from there.\n",
    "    * Apply Silhouette to the benchmark-assigned cell types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature ranking dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mereu_dir = os.path.dirname(os.getcwd()) + '/data/Mereu_2020/'\n",
    "\n",
    "for libprep in tqdm(['10X', 'CELseq2', 'ddSEQ', 'Dropseq', 'inDrop', 'QUARTZseq', 'SingleNuclei', 'SMARTseq2']):\n",
    "    for org in ['human', 'mouse']:\n",
    "        if os.path.exists(save_dir + f'mereu_{libprep}_{org}-log_feature_values.csv'):\n",
    "            print(f'{libprep}, {org} exists!')\n",
    "        else:\n",
    "            adata_libprep = sc.read(mereu_dir + f'{libprep}_{org}.h5ad')\n",
    "            create_df_feature_ranking(adata_libprep, f'mereu_{libprep}_{org}-log', apply_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ding_dir = os.path.dirname(os.getcwd()) + '/data/Ding_2020/'\n",
    "\n",
    "for libprep in tqdm(['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNA-seq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2']):\n",
    "    for org in ['human', 'mouse']:\n",
    "        if os.path.exists(ding_dir + f'{libprep}_{org}.h5ad'):\n",
    "            if os.path.exists(save_dir + f'ding_{libprep}_{org}-log_feature_values.csv'):\n",
    "                print(f'{libprep}, {org} exists!')\n",
    "            else:\n",
    "                adata_libprep = sc.read(ding_dir + f'{libprep}_{org}.h5ad')\n",
    "                create_df_feature_ranking(adata_libprep, f'ding_{libprep}_{org}-log', apply_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_ARI_silhouette_rem(lib_prep, org, seed, lab, adata_dir, save_dir):\n",
    "    if os.path.exists(adata_dir + f'{lib_prep}_{org}.h5ad'):\n",
    "#         if os.path.exists(save_dir + f'{lab}_{lib_prep}-log_{org}_comparison-scores_seed-{seed}.csv'):\n",
    "#             print(f'{lib_prep}, {org}, {seed} exists!')\n",
    "#         else:\n",
    "            adata = sc.read_h5ad(adata_dir + f'{lib_prep}_{org}.h5ad')\n",
    "            cell_type = 'cell_types' if 'cell_types' in adata.obs else 'CellType' # Somwhere I've fucked up with column name. Don't care where honestly.\n",
    "            df_rank = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{lib_prep}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            biological_silhouette_ARI_table(adata, df_rank, outdir=save_dir, file_root=f'{lab}_{lib_prep}_{org}-log', seed=seed, \n",
    "                                                        cell_types_col=cell_type, n_procs=1)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mereu's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Mereu_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq'] \n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True, num_cpus=8)\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'mereu', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ding's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Ding_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNAseq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2']\n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True, num_cpus=8)\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'ding', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['NMI'], figsize=(16, 4), title=f'NMI on {lab} datasets (log)', \n",
    "                                  filename=f'{lab}-NMI-log')\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['ARI'], figsize=(16, 4), title=f'ARI on {lab} datasets (log)', \n",
    "                                  filename=f'{lab}-ARI-log')\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_bench_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on UMAP projection (log)',\n",
    "                                   filename=f'{lab}-silhouette_UMAP_celltypes-log')\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_leiden_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on UMAP projection (log)', \n",
    "                                  filename=f'{lab}-silhouette_UMAP_leiden-log')\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on PCA projection (log)', \n",
    "                                  filename=f'{lab}-silhouette_PCA_celltypes-log')\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on PCA projection (log)', \n",
    "                                  filename=f'{lab}-silhouette_PCA_leiden-log')\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['DavBo_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, cell types on PCA projection [less is better] (log)', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_celltypes-log', increasing=True)\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['DavBo_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, leiden clusters on PCA projection [less is better] (log)', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_leiden-log', increasing=True)\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_bench_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on selected features (log)',\n",
    "                                   filename=f'{lab}-silhouette_selected features_celltypes-log')\n",
    "    plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_leiden_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on selected features (log)', \n",
    "                                  filename=f'{lab}-silhouette_selected features_leiden-log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['NMI'], figsize=(16, 4), title=f'NMI on {lab} datasets (log and nonlog)', \n",
    "                                  filename=f'{lab}-NMI-both')\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['ARI'], figsize=(16, 4), title=f'ARI on {lab} datasets (log and nonlog)', \n",
    "                                  filename=f'{lab}-ARI-both')\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on UMAP projection (log and nonlog)',\n",
    "                                   filename=f'{lab}-silhouette_UMAP_celltypes-both')\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_UMAP'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on UMAP projection (log and nonlog)', \n",
    "                                  filename=f'{lab}-silhouette_UMAP_leiden-both')\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on PCA projection (log and nonlog)', \n",
    "                                  filename=f'{lab}-silhouette_PCA_celltypes-both')\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on PCA projection (log and nonlog)', \n",
    "                                  filename=f'{lab}-silhouette_PCA_leiden-both')\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_bench_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, cell types on PCA projection [less is better] (log and nonlog)', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_celltypes-both', increasing=True)\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['DavBo_leiden_PCA'], figsize=(16, 4), \n",
    "                                   title=f'Davies-Bouldin on {lab} datasets, leiden clusters on PCA projection [less is better] (log and nonlog)', \n",
    "                                  filename=f'{lab}-Davies-Bouldin_PCA_leiden-both', increasing=True)\n",
    "    \n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_bench_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, cell types on selected features (log and nonlog)',\n",
    "                                   filename=f'{lab}-silhouette_selected features_celltypes-both')\n",
    "    plot_lab_org_comparison_scores(lab, org='-', read_dir=save_dir, variables=['Sil_leiden_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on selected features (log and nonlog)', \n",
    "                                  filename=f'{lab}-silhouette_selected features_leiden-both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, generally, all types of scores are unaltered by log transformation. Interestingly, in some datasets (ding SMARTseq2 human, Seq-Well human, 10X mouse; mereu inDrop mouse, CELseq2 human, SingleNuclei human) either NMI or Davies-Bouldin / Silhouette scores worsen in nbumi and/or brennecke methods. Regarding triku, log-transformation keeps the scores unaltered. Therefore, we can safely apply triku in either log-transformed or raw data, and cluster metric will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['mereu', 'ding']:\n",
    "    list_files = [i for i in os.listdir(save_dir) if lab in i and \"comparison-scores\" in i and 'log' not in i]\n",
    "    list_files_log = [i for i in os.listdir(save_dir) if lab in i and \"comparison-scores\" in i and 'log' in i]\n",
    "\n",
    "    for var, title in zip(\n",
    "        ['ARI', 'NMI', 'Sil_bench_UMAP', 'Sil_leiden_UMAP', 'Sil_bench_PCA', 'Sil_leiden_PCA', 'Sil_bench_all_hvg', 'Sil_leiden_all_hvg'], \n",
    "        ['Rank comparison of ' + i for i in \n",
    "         ['ARI', 'NMI', 'Silhouette of cell types in UMAP', 'Silhouette of leiden clusters in UMAP', \n",
    "          'Silhouette of cell types in PCA', 'Silhouette of leiden clusters in PCA', \n",
    "          'Silhouette of cell types in selected features', 'Silhouette of leiden clusters in selected features']]):\n",
    "        \n",
    "        \n",
    "        compare_rankings(\n",
    "            list_files,\n",
    "            list_files_log,\n",
    "            read_dir=save_dir,\n",
    "            title=f'{title} - {lab}',\n",
    "            title1=\"Non log\",\n",
    "            title2=\"Log\",\n",
    "            variables=[var],\n",
    "            increasing=False,\n",
    "            alpha=0.05,\n",
    "            figsize=(4, 5),\n",
    "            mode=\"normal\",\n",
    "            filename=f\"rank_comparison-{lab}-{title.replace(' ', '_')}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['mereu', 'ding']:\n",
    "    list_files = sorted([i for i in os.listdir(save_dir) if lab in i and \"comparison-scores\" in i and 'log' not in i])\n",
    "    list_files_log = sorted([i for i in os.listdir(save_dir) if lab in i and \"comparison-scores\" in i and 'log' in i])\n",
    "\n",
    "    for var, title in zip(\n",
    "        ['ARI', 'NMI', 'Sil_bench_UMAP', 'Sil_leiden_UMAP', 'Sil_bench_PCA', 'Sil_leiden_PCA', 'Sil_bench_all_hvg', 'Sil_leiden_all_hvg'], \n",
    "        ['log - nonlog differences on ' + i for i in \n",
    "         ['ARI', 'NMI', 'Silhouette of cell types in UMAP', 'Silhouette of leiden clusters in UMAP', \n",
    "          'Silhouette of cell types in PCA', 'Silhouette of leiden clusters in PCA', \n",
    "          'Silhouette of cell types in selected features', 'Silhouette of leiden clusters in selected features']]):\n",
    "                \n",
    "        compare_values(\n",
    "            list_files_log,\n",
    "            list_files,\n",
    "            read_dir=save_dir,\n",
    "            title=f\"{title} - {lab}\",\n",
    "            variables=[var],\n",
    "            figsize=(10, 4),\n",
    "            filename=f\"difference-{lab}-{title.replace(' ', '_')}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability of results\n",
    "Although we see that triku has promising results, we were striked at how std, scry and brennecke have such a big gap of scores with respect to triku, nbumi and m3drop. In this section we are going to apply some comprobation measures to see if we can know why the difference is so big. \n",
    "\n",
    "Some comprobations we are going to do are:\n",
    "* Plot UMAPs of datasets with observed cell types and leiden solutions.\n",
    "* Plot CV^2 / std / percentage of zeros VS mean plots to see biases on each method.\n",
    "* Plot heatmaps with percentage of overlap between pairs of techniques to see similarities. We expect triku + nbumi + m3drop to have higher similarity and brennecke + std + scry also to be similar together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exports_dir = os.getcwd() + '/exports/comparisons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WARNING!!! create_UMAP_adataset_libprep_org has harcoded feature selection method names!!\n",
    "dict_UMAP_info_mereu = create_dict_UMAPs_datasets(adata_dir=data_dir + 'Mereu_2020/', df_rank_dir=exports_dir, lab='mereu', \n",
    "                                                  lib_preps=['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq'], \n",
    "                                                  list_orgs = ['human', 'mouse'], log=True)\n",
    "\n",
    "f = open(f\"{exports_dir}/dict_info_mereu.pkl\",\"wb\")\n",
    "pickle.dump(dict_UMAP_info_mereu,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_UMAP_info_ding = create_dict_UMAPs_datasets(adata_dir=data_dir + 'Ding_2020/', df_rank_dir=exports_dir, lab='ding', \n",
    "                                                  lib_preps=['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNAseq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2'], \n",
    "                                                  list_orgs = ['human', 'mouse'], log=True)\n",
    "\n",
    "f = open(f\"{exports_dir}/dict_info_ding.pkl\",\"wb\")\n",
    "pickle.dump(dict_UMAP_info_ding,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{exports_dir}/dict_info_mereu.pkl\", 'rb') as f:\n",
    "    dict_UMAP_info_mereu = pickle.load(f)\n",
    "\n",
    "with open(f\"{exports_dir}/dict_info_ding.pkl\", 'rb') as f:\n",
    "    dict_UMAP_info_ding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_UMAPs_datasets(dict_UMAP_info_mereu, os.getcwd() + '/figures/comparison_figs', lab='mereu', figsize=(20, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_UMAPs_datasets(dict_UMAP_info_ding, os.getcwd() + '/figures/comparison_figs', lab='ding', figsize=(20, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_XY(dict_returns, x_var, y_var, fig_save_dir, lab, figsize=(20, 35), logx=True, logy=True, title=''):\n",
    "    list_rows = list(dict_returns.keys())\n",
    "    list_methods = list(dict_returns[list_rows[0]].keys())[:-1]\n",
    "        \n",
    "    fig, axs = plt.subplots(len(list_rows), len(list_methods), figsize=figsize)\n",
    "    \n",
    "    for row_idx, row_name in enumerate(list_rows):\n",
    "        for col_idx, col_name in enumerate(list_methods):\n",
    "            x_coords = dict_returns[row_name]['other_stuff'][x_var]\n",
    "            y_coords = dict_returns[row_name]['other_stuff'][y_var]\n",
    "            highly_variable = dict_returns[row_name][col_name]['highly_variable']\n",
    "            \n",
    "            if logx: \n",
    "                x_coords = np.log10(x_coords)\n",
    "            if logy: \n",
    "                y_coords = np.log10(y_coords)\n",
    "            \n",
    "            axs[row_idx][col_idx].scatter(x_coords[highly_variable == False], y_coords[highly_variable == False], c = '#cbcbcb', alpha=0.05, s=2)\n",
    "            axs[row_idx][col_idx].scatter(x_coords[highly_variable == True], y_coords[highly_variable == True], c = '#007ab7', alpha=0.2, s=2)\n",
    "                       \n",
    "            if row_idx == 0:\n",
    "                axs[row_idx][col_idx].set_title(f\"{col_name}\")\n",
    "                \n",
    "            if col_idx == 0:\n",
    "                axs[row_idx][col_idx].set_ylabel(f\"{row_name}\".replace(' ', '\\n'))\n",
    "                axs[row_idx][col_idx].yaxis.set_label_position('left') \n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for fmt in ['png', 'svg']:\n",
    "        os.makedirs(f'{fig_save_dir}/{fmt}', exist_ok=True)\n",
    "        fig.savefig(f'{fig_save_dir}/{fmt}/{lab}_{x_var}-VS-{y_var}.{fmt}', bbox_inches='tight')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_mereu, 'mean', 'per_0', lab='mereu', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=False, \n",
    "       title='Mereu, percentage of zeros VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_mereu, 'mean', 'std', lab='mereu', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=True, \n",
    "       title='Mereu, log std VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_mereu, 'mean', 'CV2', lab='mereu', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=True, \n",
    "       title='Mereu, log CV2 VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_mereu, 'mean', 'disp', lab='mereu', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=False, \n",
    "       title='Mereu, dispersion (scanpy) VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_ding, 'mean', 'per_0', lab='ding', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=False, \n",
    "       title='ding, percentage of zeros VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_ding, 'mean', 'std', lab='ding', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=True, \n",
    "       title='ding, log std VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_ding, 'mean', 'CV2', lab='ding', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=True, \n",
    "       title='ding, log CV2 VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_XY(dict_UMAP_info_ding, 'mean', 'disp', lab='ding', fig_save_dir=os.getcwd() + '/figures/comparison_figs', logx=True, logy=False, \n",
    "       title='ding, dispersion (scanpy) VS log mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps_jaccard(dict_info, size_factors=(5, 5), fig_save_dir='', lab=''):\n",
    "    import math \n",
    "    import seaborn as sns\n",
    "    \n",
    "    n_datasets = len(dict_info)\n",
    "    n_cols = min(4, int(n_datasets ** 0.5))\n",
    "    n_rows = math.ceil(n_datasets / n_cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_rows * size_factors[0], n_cols * size_factors[1]))\n",
    "    \n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            sq_number = col + n_cols * row\n",
    "            \n",
    "            if sq_number < n_datasets:\n",
    "            \n",
    "                dataset_dict = dict_info[list(dict_info.keys())[sq_number]]\n",
    "\n",
    "                methods = list(dataset_dict.keys())[:-1]\n",
    "\n",
    "                array_heatmap = np.empty((len(methods), len(methods)))\n",
    "                array_heatmap[:] = np.NaN\n",
    "\n",
    "                for i, method_i in enumerate(methods):\n",
    "                    for j, method_j in enumerate(methods):\n",
    "                        if i >= j:\n",
    "                            hvg_i, hvg_j = dataset_dict[method_i]['highly_variable'], dataset_dict[method_j]['highly_variable']\n",
    "                            jaccard = np.sum(hvg_i & hvg_j)/np.sum(hvg_i | hvg_j)\n",
    "                            array_heatmap[i, j] = jaccard\n",
    "\n",
    "                h = sns.heatmap(array_heatmap, cbar=False, ax=axs[row][col], annot=True, xticklabels=methods, yticklabels=methods)\n",
    "                h.set_title(list(dict_info.keys())[sq_number])\n",
    "            \n",
    "            else:\n",
    "                axs[row][col].set_frame_on(False)\n",
    "                axs[row][col].get_xaxis().set_visible(False)\n",
    "                axs[row][col].get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    for fmt in ['png', 'pdf']:\n",
    "        os.makedirs(f'{fig_save_dir}/{fmt}', exist_ok=True)\n",
    "        fig.savefig(f'{fig_save_dir}/{fmt}/{lab}_heatmap_overlap_features.{fmt}', bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_jaccard(dict_UMAP_info_mereu, size_factors=(2.7, 2.7*3), fig_save_dir=os.getcwd() + '/figures/comparison_figs', lab='mereu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_jaccard(dict_UMAP_info_ding, size_factors=(4.3, 4.3), fig_save_dir=os.getcwd() + '/figures/comparison_figs', lab='ding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of ribosomal and mitochondrial genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_mt_rbp(lab, org, method, n_features=[100, 250, 500, 1000], mode=0):\n",
    "    list_FS = ['triku_0', 'scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi'] # std is missing\n",
    "    palette = [\"#E73F74\",\"#7F3C8D\",\"#11A579\",\"#3969AC\",\"#F2B701\",\"#80BA5A\",\"#E68310\",\"#a0a0a0\",\"#505050\"]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    \n",
    "    for n_feature_idx, n_feature in enumerate(n_features):\n",
    "        for FS_idx, FS in enumerate(list_FS):\n",
    "            df = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            set_rbp = set([i for i in df.index if (i.upper().startswith('RPS')) | (i.upper().startswith('RPL'))])\n",
    "            set_mt = set([i for i in df.index if (i.upper().startswith('MT-'))])\n",
    "            \n",
    "            set_FS = set(df.sort_values(by=FS).index.tolist()[:n_feature])\n",
    "            \n",
    "            if mode == 0:\n",
    "                axs[0].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_rbp & set_FS)/len(set_FS), \n",
    "                        width = 0.1, color=palette[FS_idx])\n",
    "                axs[1].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_mt & set_FS)/len(set_FS), \n",
    "                        width = 0.1, color=palette[FS_idx])\n",
    "            else:\n",
    "                axs[0].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_rbp & set_FS)/len(set_rbp), \n",
    "                        width = 0.1, color=palette[FS_idx])\n",
    "                axs[1].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_mt & set_FS)/len(set_rbp), \n",
    "                        width = 0.1, color=palette[FS_idx])\n",
    "                \n",
    "    for ax in axs:\n",
    "        ax.set_xticks(range(len(n_features)))\n",
    "        ax.set_xticklabels(n_features)\n",
    "    \n",
    "    if mode == 0:\n",
    "        axs[0].set_ylabel('% ribosomal genes\\n(from selected features)')\n",
    "        axs[1].set_ylabel('% mitochondrial genes\\n(from selected features)')\n",
    "    else:\n",
    "        axs[0].set_ylabel('% ribosomal genes\\n(from all ribosomal genes)')\n",
    "        axs[1].set_ylabel('% mitochondrial genes\\n(from all mitochondrial genes)')\n",
    "        \n",
    "    legend_elements = [mpl.lines.Line2D([0], [0], marker=\"o\", color=palette[0], label='triku')] + [\n",
    "        mpl.lines.Line2D(\n",
    "            [0], [0], marker=\"o\", color=palette[j], label=list_FS[j]\n",
    "        )\n",
    "        for j in range(1, len(list_FS))\n",
    "    ]\n",
    "    axs[0].legend(handles=legend_elements, bbox_to_anchor=(1.2, 0.9))\n",
    "    \n",
    "    \n",
    "def heatmap_mt_rbp(labs, orgs, methods, n_features=500):\n",
    "    list_FS = ['triku_0', 'm3drop', 'nbumi', 'scanpy', 'std', 'scry', 'brennecke', ] # std is missing\n",
    "    palette = [\"#E73F74\",\"#80BA5A\",\"#E68310\",\"#7F3C8D\",\"#11A579\",\"#3969AC\",\"#F2B701\",\"#a0a0a0\",\"#505050\"]\n",
    "        \n",
    "    dict_info = {}\n",
    "    \n",
    "    for lab in labs:\n",
    "        for org in orgs:\n",
    "            for method in methods:\n",
    "                for FS_idx, FS in enumerate(list_FS):\n",
    "                    if not os.path.exists(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv'):\n",
    "                        continue\n",
    "                        \n",
    "                    df = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "                    set_rbp = set([i for i in df.index if (i.upper().startswith('RPS')) | (i.upper().startswith('RPL'))])\n",
    "                    set_mt = set([i for i in df.index if (i.upper().startswith('MT-'))])\n",
    "\n",
    "                    set_FS = set(df.sort_values(by=FS).index.tolist()[:n_features])\n",
    "                    \n",
    "                    for opt in [f'{FS}_per_rbp_all_features', f'{FS}_per_rbp_rbp_features', \n",
    "                                f'{FS}_per_mt_all_features', f'{FS}_per_mt_mt_features']:\n",
    "                        if opt not in dict_info:\n",
    "                            dict_info[opt] = []\n",
    "                    \n",
    "                    dict_info[f'{FS}_per_rbp_all_features'].append(100 * len(set_rbp & set_FS) / len(set_FS))\n",
    "                    dict_info[f'{FS}_per_rbp_rbp_features'].append(100 * len(set_rbp & set_FS) / len(set_rbp))\n",
    "                    dict_info[f'{FS}_per_mt_all_features'].append(100 * len(set_mt & set_FS) / len(set_FS))\n",
    "                    dict_info[f'{FS}_per_mt_mt_features'].append(100 * len(set_mt & set_FS) / len(set_mt))\n",
    "    \n",
    "    df = pd.DataFrame(index=['triku'] + list_FS[1:], columns=[\n",
    "        'Percentage RBPs in selected features', f'Percentage RBPs from RBP gene list ({len(set_rbp)} elements)', \n",
    "        'Percentage MTs in selected features', f'Percentage MTs from MT gene list ({len(set_mt)} elements)'])\n",
    "    \n",
    "    for FS_idx, FS in enumerate(list_FS):\n",
    "        df.iloc[FS_idx, 0] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_rbp_all_features'])\n",
    "        df.iloc[FS_idx, 1] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_rbp_rbp_features']) \n",
    "        df.iloc[FS_idx, 2] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_mt_all_features'])\n",
    "        df.iloc[FS_idx, 3] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_mt_mt_features'])\n",
    "                                \n",
    "                                     \n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = heatmap_mt_rbp(['ding'], ['human', 'mouse'], ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', \n",
    "                                              'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], n_features=1000)\n",
    "\n",
    "display(df)\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = heatmap_mt_rbp(['mereu'], ['human', 'mouse'], ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', \n",
    "                                              'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], n_features=1000)\n",
    "\n",
    "display(df)\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab, org, method, n_features = 'ding', 'human', 'Dropseq', [100, 250, 500, 1000]\n",
    "\n",
    "barplot_mt_rbp(lab, org, method, n_features=[100, 250, 500, 1000], mode=0)\n",
    "barplot_mt_rbp(lab, org, method, n_features=[100, 250, 500, 1000], mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene ontology analysis\n",
    "\n",
    "To see which method is better, a possible idea is to run Enrichr with the selected features, and use it to compare the FS methods. If the ontologies from one method have better p-values/scores, it is likely that they are more representative of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + f'/exports/enrichr/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_onto_mouse = ['KEGG_2019_Mouse', 'WikiPathways_2019_Mouse', 'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', \n",
    "                   'GO_Molecular_Function_2018',]\n",
    "\n",
    "list_onto_human = ['KEGG_2019_Human', 'WikiPathways_2019_Human', 'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', \n",
    "                   'GO_Molecular_Function_2018', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def call_enrichr(lab, org, method, n_features, FS):\n",
    "    if os.path.exists(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv'):\n",
    "        print(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv EXISTS!')\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv'):\n",
    "        return None\n",
    "    \n",
    "    df_file = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "    \n",
    "    list_genes = df_file.sort_values(by=FS).index.tolist()[:n_features]\n",
    "    list_onto = list_onto_mouse if org == 'mouse' else list_onto_human\n",
    "    \n",
    "    n_trials = 0\n",
    "    \n",
    "    while n_trials < 5:\n",
    "        try:\n",
    "            result_df = gseapy.enrichr(list_genes, list_onto, cutoff=1, organism=org).results\n",
    "            if FS == 'triku_0':\n",
    "                FS = 'triku'\n",
    "            result_df.to_csv(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv', index=None)\n",
    "            n_trials += 10\n",
    "        except:\n",
    "            n_trials += 1\n",
    "            print(f'TRIAL {n_trials}')\n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_comb = list(product(*[['ding', 'mereu'], \n",
    "                           ['human', 'mouse'], \n",
    "                           ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], \n",
    "                           [100, 250, 500, 1000, 1250, 1500], \n",
    "                           ['triku_0', 'scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi']]))\n",
    "\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "list_id = [call_enrichr.remote(lab, org, method, n_features, FS) for lab, org, method, n_features, FS in list_comb]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_enrichr(lab, org, method, n_features, n_ontologies=30, column_sort='Adjusted P-value', plot_type='bar', \n",
    "                    list_onto=['KEGG_2019_Mouse', 'WikiPathways_2019_Mouse', 'KEGG_2019_Human', 'WikiPathways_2019_Human',\n",
    "                               'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', 'GO_Molecular_Function_2018',], save=True):\n",
    "    list_FS = ['triku', 'scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi']\n",
    "    palette = [\"#E73F74\",\"#7F3C8D\",\"#11A579\",\"#3969AC\",\"#F2B701\",\"#80BA5A\",\"#E68310\",\"#a0a0a0\",\"#505050\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    \n",
    "    dict_dfs = {}\n",
    "    \n",
    "    for n_feature_idx, n_feature in enumerate(n_features):\n",
    "        for FS_idx, FS in enumerate(list_FS):\n",
    "            df = pd.read_csv(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_feature}_{FS}.csv')\n",
    "            df = df[df['Gene_set'].isin(list_onto)]\n",
    "            \n",
    "            if column_sort == 'Adjusted P-value':\n",
    "                df = df.sort_values(by=column_sort).iloc[:n_ontologies]\n",
    "                y_vals = df[column_sort].values\n",
    "                y_vals = - np.log10(y_vals)\n",
    "\n",
    "            elif column_sort == 'Combined Score':\n",
    "                df = df.sort_values(by=column_sort, ascending=False).iloc[:n_ontologies]\n",
    "                y_vals = df[column_sort].values\n",
    "            \n",
    "            elif column_sort == 'division':\n",
    "                table_vals = df['Overlap'].values\n",
    "                df['divided'] = [int(i.split('/')[0]) / int(i.split('/')[1]) for i in table_vals]\n",
    "                df = df.sort_values(by='divided', ascending=False).iloc[:n_ontologies]\n",
    "                y_vals = df['divided'].values\n",
    "                \n",
    "            \n",
    "            x_pos = n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3)\n",
    "            \n",
    "            if plot_type == 'bar':\n",
    "                plt.bar(x_pos , np.mean(y_vals), \n",
    "                        width = 0.1, yerr=np.std(y_vals), color=palette[FS_idx])\n",
    "            elif plot_type == 'scatter':\n",
    "                plt.scatter([x_pos] * len(y_vals), y_vals, c=palette[FS_idx], alpha=0.8)\n",
    "            \n",
    "            dict_dfs[f'{n_feature}_{FS}'] = df\n",
    "    \n",
    "    legend_elements = [\n",
    "        mpl.lines.Line2D(\n",
    "            [0], [0], marker=\"o\", color=palette[j], label=list_FS[j]\n",
    "        )\n",
    "        for j in range(len(list_FS))\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, bbox_to_anchor=(1.2, 0.9))\n",
    "    ax.set_xticks(range(len(n_features)))\n",
    "    ax.set_xticklabels(n_features)\n",
    "    ax.set_ylabel(column_sort)\n",
    "    ax.set_xlabel('Number of features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    \n",
    "    return dict_dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_ontologies_individual(df, axis=None, color=\"#ababab\", column='Adjusted P-value', ascending=False, log=True, y_text=''):\n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    \n",
    "    vals = df.sort_values(by=column, ascending=ascending)[column].values\n",
    "    names = [i.split(' (')[0] for i in df.sort_values(by=column, ascending=ascending)['Term'].values]\n",
    "    names = [i[: 42] + '...' if len(i) > 42 else i for i in names]\n",
    "\n",
    "    if log:\n",
    "        vals = - np.log10(df.sort_values(by=column, ascending=ascending)[column].values)\n",
    "    \n",
    "    if column == 'Adjusted P-value':\n",
    "        if log:\n",
    "            axis.plot(-np.log10([0.05, 0.05]), [-1.5, len(names) + 0.5], c=\"#ababab\", alpha=0.8, linewidth=3, zorder=0)\n",
    "        else:\n",
    "            axis.plot([0.05, 0.05], [-1.5, len(names) + 0.5], c=\"#ababab\", alpha=0.8, linewidth=3, zorder=0)\n",
    "        \n",
    "    axis.barh(range(len(df)), vals, color=color, zorder=5)\n",
    "    \n",
    "    for y in range(len(df)):\n",
    "        axis.text(0.05 * np.max(axis.get_xlim()), y - 0.2, names[y], zorder=10, fontsize=12)\n",
    "        \n",
    "    axis.set_yticks([])\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "    x_text = column if not log else column + ' (log)'\n",
    "    axis.set_xlabel(x_text)\n",
    "    axis.set_ylabel(y_text)\n",
    "    \n",
    "    return axis\n",
    "\n",
    "\n",
    "def barplot_ontologies_all(dict_dfs, n_features=1000, list_FSs=['triku', 'std', 'scry', 'scanpy', 'm3drop', 'nbumi'], \n",
    "                           list_colors=[\"#E73F74\", \"#11A579\",\"#3969AC\", \"#7F3C8D\", \"#80BA5A\",\"#E68310\"], figsize=(17, 14), save=''):\n",
    "    \n",
    "    mpl.rcParams.update({'font.size':17})\n",
    "    fig, axis = plt.subplots(2, 3, figsize=figsize)\n",
    "    \n",
    "    for i in range(len(list_FSs)):\n",
    "        barplot_ontologies_individual(dict_dfs[f'{n_features}_{list_FSs[i]}'], axis=axis.ravel()[i], \n",
    "                                      color=list_colors[i], column='Adjusted P-value', ascending=False, log=True, y_text=list_FSs[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "        \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichr_figs_dir = os.getcwd() + '/figures/enrichr_figs/'\n",
    "os.makedirs(enrichr_figs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good in Ding: Dropseq Human (Immune cells)\n",
    "\n",
    "lab, org, method, n_features = 'ding', 'human', 'Dropseq', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_ding_human_dropseq = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=[ 'GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_ding_human_dropseq.append(dict_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_ontologies_all(list_dfs_ding_human_dropseq[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good in Mereu: Dropseq Mouse (Colon cells)\n",
    "\n",
    "lab, org, method, n_features = 'mereu', 'mouse', 'Dropseq', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_mereu_mouse_dropseq = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=['GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_mereu_mouse_dropseq.append(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_ontologies_all(list_dfs_mereu_mouse_dropseq[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad in Ding: 10X Human (Immune cells)\n",
    "\n",
    "lab, org, method, n_features = 'ding', 'human', '10X', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_ding_human_10x = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=['GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_ding_human_10x.append(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_ontologies_all(list_dfs_ding_human_10x[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad in Mereu: 10X Mouse (Colon cells)\n",
    "\n",
    "lab, org, method, n_features = 'mereu', 'mouse', '10X', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_mereu_mouse_10x = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=['GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_mereu_mouse_10x.append(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_ontologies_all(list_dfs_mereu_mouse_10x[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10X datasets\n",
    "In this section we are going to analyze the 10x neuron, heart and pbmc datasets.\n",
    "We are going to show the Silhouette scores for leiden using the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_10x_dir = os.path.dirname(os.getcwd()) + '/data/10x/'\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for libprep in tqdm(['neuron', 'pbmc', 'heart', ]):\n",
    "    if os.path.exists(save_dir + f'10X-datasets_{libprep}_feature_values.csv'):\n",
    "        print(f'{libprep}, exists!')\n",
    "    else:\n",
    "        adata_libprep = sc.read_10x_h5(_10x_dir + f'{libprep}_10k_v3_raw_feature_bc_matrix.h5')\n",
    "        adata_libprep.var_names_make_unique()\n",
    "        sc.pp.filter_cells(adata_libprep, min_counts=400)\n",
    "        sc.pp.filter_genes(adata_libprep, min_counts=100)\n",
    "        adata_libprep.var_names_make_unique()\n",
    "        adata_libprep.X = np.asarray(adata_libprep.X.todense())\n",
    "        del [adata_libprep.var, adata_libprep.obs]\n",
    "\n",
    "        print(adata_libprep.X)\n",
    "        create_df_feature_ranking(adata_libprep, f'10X-datasets_nonlog_{libprep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "\n",
    "tissues = ['heart', 'pbmc', 'neuron']\n",
    "\n",
    "for tissue in tissues:\n",
    "    print(tissue)\n",
    "    adata_libprep = sc.read_10x_h5(_10x_dir + f'{tissue}_10k_v3_raw_feature_bc_matrix.h5')\n",
    "    adata_libprep.var_names_make_unique()\n",
    "    sc.pp.filter_cells(adata_libprep, min_counts=400)\n",
    "    sc.pp.filter_genes(adata_libprep, min_counts=100)\n",
    "    adata_libprep.var_names_make_unique()\n",
    "    adata_libprep.X = np.asarray(adata_libprep.X.todense())\n",
    "    df_rank = pd.read_csv(os.getcwd() + f'/exports/comparisons/10X-datasets_nonlog_{tissue}_feature_ranks.csv', index_col=0)\n",
    "    \n",
    "    for seed in range(5):\n",
    "        biological_silhouette_ARI_table(adata_libprep, df_rank, outdir=save_dir, file_root=f'10X-datasets_nonlog_{tissue}', seed=seed, \n",
    "                                                            cell_types_col=None, n_procs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for libprep in tqdm(tissues):\n",
    "    if os.path.exists(save_dir + f'10X-datasets_log_{libprep}_feature_values.csv'):\n",
    "        print(f'{libprep}, exists!')\n",
    "    else:\n",
    "        adata_libprep = sc.read_10x_h5(_10x_dir + f'{libprep}_10k_v3_raw_feature_bc_matrix.h5')\n",
    "        adata_libprep.var_names_make_unique()\n",
    "        sc.pp.filter_cells(adata_libprep, min_counts=400)\n",
    "        sc.pp.filter_genes(adata_libprep, min_counts=100)\n",
    "        adata_libprep.var_names_make_unique()\n",
    "        adata_libprep.X = np.asarray(adata_libprep.X.todense())\n",
    "        del [adata_libprep.var, adata_libprep.obs]\n",
    "        \n",
    "        sc.pp.log1p(adata_libprep)\n",
    "        \n",
    "        print(adata_libprep.X)\n",
    "        create_df_feature_ranking(adata_libprep, f'10X-datasets_log_{libprep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "\n",
    "tissues = ['heart', 'pbmc', 'neuron']\n",
    "\n",
    "for tissue in tissues:\n",
    "    print(tissue)\n",
    "    adata_libprep = sc.read_10x_h5(_10x_dir + f'{tissue}_10k_v3_raw_feature_bc_matrix.h5')\n",
    "    adata_libprep.var_names_make_unique()\n",
    "    sc.pp.filter_cells(adata_libprep, min_counts=400)\n",
    "    sc.pp.filter_genes(adata_libprep, min_counts=100)\n",
    "    adata_libprep.var_names_make_unique()\n",
    "    adata_libprep.X = np.asarray(adata_libprep.X.todense())\n",
    "        \n",
    "    sc.pp.log1p(adata_libprep)\n",
    "\n",
    "    df_rank = pd.read_csv(os.getcwd() + f'/exports/comparisons/10X-datasets_log_{tissue}_feature_ranks.csv', index_col=0)\n",
    "    \n",
    "    for seed in range(5):\n",
    "        biological_silhouette_ARI_table(adata_libprep, df_rank, outdir=save_dir, file_root=f'10X-datasets_log_{tissue}', seed=seed, \n",
    "                                                            cell_types_col=None, n_procs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = '10X-datasets'\n",
    "\n",
    "plot_lab_org_comparison_scores(lab=lab, org='-', read_dir=save_dir, variables=['Sil_leiden_PCA'], figsize=(14, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on PCA projection', \n",
    "                                  filename=f'{lab}-Silhouette_PCA_leiden')\n",
    "\n",
    "plot_lab_org_comparison_scores(lab=lab, org='-', read_dir=save_dir, variables=['Sil_leiden_all_hvg'], figsize=(14, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on selected features', \n",
    "                                  filename=f'{lab}-Silhouette_selected features_celltypes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alex-base] *",
   "language": "python",
   "name": "conda-env-alex-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
