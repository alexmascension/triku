{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why applying median correction is necessary\n",
    "\n",
    "When calculating the Wasserstein distance with triku, the next step is to plot the distances against the mean expression of each gene. We usually see that genes of interest have a higher Wasserstein distance, as expected; but what is more important, their Wasserstein distance is much higher than the median distance for genes with similar expression. It is usually common to find that genes with much higher expression have biases towards higher distances. Those genes can be useful features, but generally there are better features, and those genes \"obscure\" them, since they basaly have a higher distance. \n",
    "\n",
    "Therefore, we should \"correct\" the Wasserstein distance considering the mean expression. The main idea is to obtain a baseline level, which can be easily done by calculating the median distance across the whole gene expression values (using log is less sensitive to expressing across differente scales, or orders of magnitude).\n",
    "\n",
    "Afterwards we should remove that baseline level to each gene. We can do that in two ways:\n",
    "* A simpler but coarser approach is to simply substract the median in each window to the distance of the gene in that window. This approach produces a more stepped graph, which can introduce artifacts if the number of windows is not high enough.\n",
    "* We can adjust a cubic spline to the median in each window, and then apply the substraction to the interpolated values. This gives a smoother graph, and less prone to artifacts if the number of windows is not high enough. (Supposedly)\n",
    "\n",
    "Regarding the number of windows, since we will always be considering around 5000 - 15000 genes, a number of windows around 50 - 200 will be enough. However, in some expression ranges (towards the high expression end), if the number of genes is not high enough (since most of them are concentrated towards lower expression values). \n",
    "\n",
    "To see differences in median correction, we will first work with artificial datasets with varying degrees of differentiale expression probabilities. Then we will extrapolate this method to biological datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import triku as tk\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearColorMapper\n",
    "from bokeh.transform import dodge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triku_nb_code.palettes_and_cmaps import magma, bold_and_vivid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is already defined within triku internals, but we will include it here to\n",
    "# understand it better.\n",
    "def subtract_median(x, y, n_windows, apply_spline):\n",
    "    \"\"\"x and y are mean and distance values.\"\"\"\n",
    "    \n",
    "    # We have to take the distance in logarithm to account for the wide expression ranges\n",
    "    linspace = 10**np.linspace(min(np.log10(x)), max(np.log10(x)), n_windows + 1)\n",
    "    y_adjust = y.copy()\n",
    "    \n",
    "    if apply_spline:\n",
    "        median_x, median_y = [], []\n",
    "       \n",
    "        for i in range(n_windows):\n",
    "            mask = (x >= linspace[i]) & (x <= linspace[i + 1])\n",
    "            if np.any(mask): # If the mask is empty there's no need to add anything because there are no genes\n",
    "                median_y.append(np.median(y[mask]))\n",
    "                median_x.append(np.median(x[mask]))      \n",
    "        \n",
    "        spline = interpolate.splrep(np.array(median_x), np.array(median_y), s=0)\n",
    "        y_adjust -= interpolate.splev(x, spline)\n",
    "        \n",
    "        return y_adjust, spline\n",
    "    else:\n",
    "        y_median_array = np.zeros(len(y))\n",
    "        for i in range(n_windows):\n",
    "            mask = (x >= linspace[i]) & (x <= linspace[i + 1])\n",
    "            y_median_array[mask] = np.median(y[mask])\n",
    "        \n",
    "        y_adjust -= y_median_array\n",
    "\n",
    "        return y_adjust, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is spline approximation better than step functions?\n",
    "To answer that question we will take an artifitial dataset, and see how it behaves in a certain number of windows. We will use a low - mid range of windows (5 to 70), and a high range (100 - 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_windows_spline_step(list_windows, x, y):\n",
    "    y_adj_spli_list, spline_list, y_adj_step_list = [], [], []\n",
    "    \n",
    "    for window in list_windows:\n",
    "        y_adjs_w_spline, spline_w = subtract_median(x, y, n_windows=window, apply_spline=True)\n",
    "        y_adjs_w_step, _ = subtract_median(x, y, n_windows=window, apply_spline=False)\n",
    "        \n",
    "        y_adj_spli_list.append(y_adjs_w_spline)\n",
    "        spline_list.append(spline_w)\n",
    "        y_adj_step_list.append(y_adjs_w_step)\n",
    "        \n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axs[0].scatter(np.log10(x), y, c = \"#bcbcbc\", s=4)\n",
    "    axs[1].scatter(np.log10(x), y, c = \"#bcbcbc\", s=4)\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('log$_{10}$(mean expression)')\n",
    "        ax.set_ylabel('Wasserstein distance')\n",
    "    \n",
    "    for w_idx in range(len(list_windows)):\n",
    "        axs[0].plot(np.sort(np.log10(x)), interpolate.splev(np.sort(x), spline_list[w_idx]), \n",
    "                    alpha=0.65, label=list_windows[w_idx])\n",
    "        axs[1].plot(np.sort(np.log10(x)), np.sort(y-y_adj_step_list[w_idx]), \n",
    "                    alpha=0.65, label=list_windows[w_idx])\n",
    "        axs[0].legend(); axs[1].legend(); \n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig, axs2 = plt.subplots(len(list_windows), 2, figsize=(6, 3 * len(list_windows)))\n",
    "    for w_idx in range(len(list_windows)):\n",
    "        axs2[w_idx][0].scatter(np.log10(x), y - interpolate.splev(x, spline_list[w_idx]), s=1, \n",
    "                               label=\"Splin, W=%s\"%list_windows[w_idx])\n",
    "        axs2[w_idx][1].scatter(np.log10(x), y_adj_step_list[w_idx],  s=1, \n",
    "                              label=\"Step, W=%s\"%list_windows[w_idx])\n",
    "        axs2[w_idx][0].legend(); axs2[w_idx][1].legend()\n",
    "        \n",
    "    for ax in axs2.ravel():\n",
    "        ax.set_xlabel('log$_{10}$(mean expression)')\n",
    "        ax.set_ylabel('Wasserstein distance')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_splatter_005 = sc.read(dataset_dir + 'splatter/splatter_deprob_0.005.loom') #, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tl.triku(adata_splatter_005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (np.asarray(adata_splatter_005.X.todense())).mean(0)\n",
    "y = adata_splatter_005.var['triku_distance_uncorrected'].values -  adata_splatter_005.var['triku_distance_random'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([15, 25, 50, 70], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([100, 500, 1000, 5000], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very low ranges (5 - 15/20), there are artifacts either in steps or in splines. In splines, the spline tends to go higher in the higher expression end, which is expected. As a result, distances on the higher end are lower than what they should be. Steps, on the other hand, produce an *stepping* artifact, that is, too many genes are selected in one window and, after substraction, it creates a zigzaggy pattern.\n",
    "\n",
    "These two problems disappear with 50 - 70 windows, and the *corrected* distance pattern looks really similar with both methods. \n",
    "\n",
    "In the higher ends, opposited as expected, spline functions are erratic, and although they do not seem really apparent in the corrected scatter plots, there are some variations at 1000 and 5000 windows. However, we don't care about that number, so it is not that important.\n",
    "\n",
    "We will now apply the same to two biological datasets: 10X, and a single cell skin dataset from Tabib et al. (Tabib 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_neuron10k = sc.read_10x_h5(dataset_dir + '10x/neuron_10k_v3_filtered_feature_bc_matrix.h5')\n",
    "adata_neuron10k.var_names_make_unique()\n",
    "sc.pp.filter_genes(adata_neuron10k, min_cells=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tl.triku(adata_neuron10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (np.asarray(adata_neuron10k.X.todense())).mean(0)\n",
    "y = adata_neuron10k.var['triku_distance_uncorrected'].values -  adata_neuron10k.var['triku_distance_random'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([15, 25, 50, 70], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([100, 500, 1000, 5000], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we see that there is not much difference across windows or between methods. At 500 - 5000 windows, either splines and steps start producing artifacts at a log mean expresion of -2 to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabib_2018 = sc.read_csv(dataset_dir + 'Tabib_2018/Skin_6Control_rawUMI.csv').transpose()\n",
    "tabib_2018.var_names_make_unique()\n",
    "sc.pp.filter_genes(tabib_2018, min_cells=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabib_2018.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tl.triku(tabib_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tabib_2018.X.mean(0)\n",
    "y = tabib_2018.var['triku_distance_uncorrected'].values -  tabib_2018.var['triku_distance_random'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([15, 25, 50, 70], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_windows_spline_step([100, 500, 1000, 5000], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on we will use the step method to correct distances. In the end, the method is less convoluted, much easier to explain, less prone to fail (because it does not depend on other dependencies) and thus much easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is it important to apply this correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally se biases on models. In this caseeither biological or artificial datasets tend to have higher distances at highly-expressed genes. This phenomenom is clearly exaggerated with artificial datasets. However, we cannot be sure if this can also happen to biological datasets, that is, if highly-expressed genes can shadow other genes with lesser expression but maybe more important.\n",
    "\n",
    "To cuantify this effect we will first run triku in an artificial dataset, and see the difference on separation of clusters before and after correcting for median expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this dataset we begin to see cluster separation, but is not fully resolved.\n",
    "adata_splatter_01 = sc.read(dataset_dir + 'splatter/splatter_deprob_0.01.loom') \n",
    "adata_splatter_01.obs['groupn'] = [i.replace('Group', '') for i in adata_splatter_01.obs['Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tk.tl.triku(adata_splatter_01, n_windows=100, s=0, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_uncorrected = adata_splatter_01.var['triku_distance_uncorrected'] - adata_splatter_01.var['triku_distance_random']\n",
    "n_features = np.sum(adata_splatter_01.var['highly_variable'].values)\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "highly_variable_uncorrected = dist_uncorrected > cutoff\n",
    "highly_variable_corrected = adata_splatter_01.var['highly_variable']\n",
    "color = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(highly_variable_uncorrected)):\n",
    "    if highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif highly_variable_corrected[i] and not highly_variable_uncorrected[i]:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Corrected')\n",
    "    elif not highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Uncorrected')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh = pd.DataFrame({\n",
    "    'm': np.log10(np.asarray(adata_splatter_01.X.todense()).mean(0)),\n",
    "    'z': (np.asarray(adata_splatter_01.X.todense()) == 0).sum(0) / adata_splatter_01.shape[0],\n",
    "    'n': adata_splatter_01.var_names.values,\n",
    "    'd': adata_splatter_01.var[\"triku_distance_uncorrected\"],\n",
    "    'e': adata_splatter_01.var[\"triku_distance_uncorrected\"] - adata_splatter_01.var[\"triku_distance_random\"],\n",
    "    'e_correct': adata_splatter_01.var[\"triku_distance\"],\n",
    "    'color': color, 'label':labels\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@e')])\n",
    "\n",
    "p.scatter('m', 'e_correct', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@e')])\n",
    "\n",
    "p.scatter('m', 'e', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, before correction, most of the selected genes were biased towards a high expression, and that's all. Many interesting genes expressed in the 0 to 1 range were note selected, even if their \"relative distance\" to the median was higher. We will now see why those genes are important.\n",
    "\n",
    "To see that correcting for the median is necessary in this case, we will use two methods (that we'll use later in other notebooks) to see how good median correction is:\n",
    "* We will calculate UMAPs with highly variable genes / features. We will also include HVGs from scanpy although these comparisons will be done in other notebooks.\n",
    "* We will calculate the first 10 differentially expressed genes for each group and see how many of the features overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = np.sum(adata_splatter_01.var['highly_variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_splatter_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Default \n",
    "adata_sample = adata_splatter_01.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.highly_variable_genes(adata_sample, n_top_genes=n_features)\n",
    "default_HVG = adata_sample.var['highly_variable'].values\n",
    "sc.pp.pca(adata_sample, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "sc.pl.umap(adata_sample, color='groupn', ax=axs[0], show=False, legend_loc='on data')\n",
    "\n",
    "# Using uncorrected distances\n",
    "adata_sample = adata_splatter_01.copy()\n",
    "dist_uncorrected = adata_sample.var['triku_distance_uncorrected'] - adata_sample.var['triku_distance_random']\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "adata_sample.var['highly_variable'] = dist_uncorrected >= cutoff\n",
    "\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "sc.pl.umap(adata_sample, color='groupn', ax=axs[1], show=False, legend_loc='on data')\n",
    "\n",
    "# Using corrected distances (default)\n",
    "adata_sample = adata_splatter_01.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "sc.pl.umap(adata_sample, color='groupn', ax=axs[2], legend_loc='on data')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that either scanpy's HVG and uncorrected features can only separate the secnd group from the rest of the groups (uncorrected triku can start to separate the third group, but only slightly). After correction, the first 4 groups are separated, which is far better than the initial case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(adata_splatter_01, groupby='groupn', method='t-test', n_genes=10)\n",
    "sc.pl.rank_genes_groups_tracksplot(adata_splatter_01, dendrogram=False, n_genes=10)\n",
    "sc.pl.rank_genes_groups_dotplot(adata_splatter_01, dendrogram=False, n_genes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEGs = adata_splatter_01.uns['rank_genes_groups']['names']\n",
    "DEGs = np.asarray(DEGs.tolist()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Percentage of DEGs that are corrected features: {}'.format(np.sum(np.array([gene in adata_sample.var_names[highly_variable_corrected] for gene in DEGs]))))\n",
    "print('Percentage of DEGs that are uncorrected features: {}'.format(np.sum(np.array([gene in adata_sample.var_names[highly_variable_uncorrected] for gene in DEGs]))))\n",
    "print('Percentage of DEGs that are scanpy features: {}\\n'.format(np.sum(np.array([gene in adata_sample.var_names[default_HVG] for gene in DEGs]))))\n",
    "\n",
    "\n",
    "for gene in DEGs:\n",
    "    print(gene + ' ' * (9 - len(gene)), gene in adata_sample.var_names[highly_variable_corrected], gene in adata_sample.var_names[highly_variable_uncorrected], gene in adata_sample.var_names[default_HVG])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the proportion of genes shared after correction is three times higher, which explains the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_DEG = [i in DEGs for i in adata_sample.var_names]\n",
    "highly_variable_corrected = adata_sample.var['highly_variable']\n",
    "color = []\n",
    "labels = []\n",
    "alpha = [0.8 if highly_variable_corrected[i] or is_DEG[i] else 0.4 for i in range(len(highly_variable_uncorrected))]\n",
    "\n",
    "for i in range(len(highly_variable_uncorrected)):\n",
    "    if highly_variable_corrected[i] and is_DEG[i]:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif highly_variable_corrected[i] and not is_DEG[i]:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Corrected')\n",
    "    elif not highly_variable_corrected[i] and is_DEG[i]:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('DEG')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh = pd.DataFrame({\n",
    "    'm': np.log10(np.asarray(adata_splatter_01.X.todense()).mean(0)),\n",
    "    'z': (np.asarray(adata_splatter_01.X.todense()) == 0).sum(0) / adata_splatter_01.shape[0],\n",
    "    'n': adata_splatter_01.var_names.values,\n",
    "    'd': adata_splatter_01.var[\"triku_distance_uncorrected\"],\n",
    "    'e': adata_splatter_01.var[\"triku_distance_uncorrected\"] - adata_splatter_01.var[\"triku_distance_random\"],\n",
    "    'e_correct': adata_splatter_01.var[\"triku_distance\"],\n",
    "    'color': color, 'label':labels, 'alpha':alpha\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@e')])\n",
    "\n",
    "p.scatter('m', 'e_correct', source=df_bokeh,\n",
    "           line_color=None,\n",
    "         color='color', legend_group='label', alpha='alpha')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_sample, color=['groupn', 'Gene2119', 'Gene8506', 'Gene191', 'Gene8599', 'Gene6027'], \n",
    "           legend_loc='on data', cmap=magma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even see that some of the genes that were not selected as DEGs have indeed different expression pattern in other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x biological dataset: PBMC 10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll repeat the process with the 10X BPMC 10k dataset. Since in this case we don't have a ground truth to compare to, we will compare patterns of clusters and DEGs to see improvements in cluster detection. The rationale behind this method is to run UMAP using each of sets of features (scanpy's HVG, triku befor and after median correction), run leiden on scanpy's HVG, the map the clustering solution to the rest of the clusters, and vice-versa. We will use the same resolution for all cases, so that the only free variable is the feature selection. We should expect clusters in scanpy's leiden solution to be less informative than in triku's HVG. We compare triku after correction with scanpy, and not with triku before correction because triku before correction and scanpy show similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this dataset we begin to see cluster separation, but is not fully resolved.\n",
    "adata_PBMC_10k = sc.read_10x_h5(dataset_dir + '10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5') \n",
    "sc.pp.filter_genes(adata_PBMC_10k, min_cells=10)\n",
    "adata_PBMC_10k.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_return = tk.tl.triku(adata_PBMC_10k, n_windows=100, s=0, random_state=seed, verbose='triku', do_return=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_uncorrected = adata_PBMC_10k.var['triku_distance_uncorrected'] - adata_PBMC_10k.var['triku_distance_random']\n",
    "n_features = np.sum(adata_PBMC_10k.var['highly_variable'].values)\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "highly_variable_uncorrected = dist_uncorrected > cutoff\n",
    "highly_variable_corrected = adata_PBMC_10k.var['highly_variable']\n",
    "color = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(highly_variable_uncorrected)):\n",
    "    if highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#000000\")\n",
    "        labels.append('Both')\n",
    "    elif highly_variable_corrected[i] and not highly_variable_uncorrected[i]:\n",
    "        color.append(\"#900020\")\n",
    "        labels.append('Corrected')\n",
    "    elif not highly_variable_corrected[i] and highly_variable_uncorrected[i]:\n",
    "        color.append(\"#007ab7\")\n",
    "        labels.append('Uncorrected')\n",
    "    else:\n",
    "        color.append(\"#bcbcbc\")\n",
    "        labels.append('None')\n",
    "\n",
    "\n",
    "df_bokeh = pd.DataFrame({\n",
    "    'm': np.log10(np.asarray(adata_PBMC_10k.X.todense()).mean(0)),\n",
    "    'z': (np.asarray(adata_PBMC_10k.X.todense()) == 0).sum(0) / adata_PBMC_10k.shape[0],\n",
    "    'n': adata_PBMC_10k.var_names.values,\n",
    "    'd': adata_PBMC_10k.var[\"triku_distance_uncorrected\"],\n",
    "    'e': adata_PBMC_10k.var[\"triku_distance_uncorrected\"] - adata_PBMC_10k.var[\"triku_distance_random\"],\n",
    "    'e_correct': adata_PBMC_10k.var[\"triku_distance\"],\n",
    "    'color': color, 'label':labels\n",
    "    })[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@e')])\n",
    "\n",
    "p.scatter('m', 'e_correct', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_right'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=400, plot_width=400, tooltips=[(\"Gene\",\"@n\"), ('Value', '@e')])\n",
    "\n",
    "p.scatter('m', 'e', source=df_bokeh,\n",
    "          alpha=0.7, line_color=None,\n",
    "         color='color', legend_group='label')\n",
    "\n",
    "p.legend.location = 'top_right'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems not to be that much of a difference, but we will see later that the selection of certain genes not in blue, but in red, improves the quality of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Default \n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.highly_variable_genes(adata_sample, n_top_genes=n_features)\n",
    "default_HVG = adata_sample.var['highly_variable'].values\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "sc.tl.leiden(adata_sample, resolution=0.8, random_state=seed)\n",
    "leiden_res_scanpy = adata_sample.obs['leiden']\n",
    "sc.pl.umap(adata_sample,  color='leiden', ax=axs[0], show=False, legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "# Using uncorrected distances\n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "dist_uncorrected = adata_sample.var['triku_distance_uncorrected'] - adata_sample.var['triku_distance_random']\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "adata_sample.var['highly_variable'] = dist_uncorrected >= cutoff\n",
    "\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "adata_sample.obs['leiden'] = leiden_res_scanpy\n",
    "sc.pl.umap(adata_sample, color='leiden', ax=axs[1], show=False, legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "# Using corrected distances (default)\n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "adata_sample.obs['leiden'] = leiden_res_scanpy\n",
    "sc.pl.umap(adata_sample, color='leiden', ax=axs[2], legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of remarkable differences:\n",
    "* Clusters  3 and 6 appear completely mixed in triku's UMAPs. \n",
    "* Cluster 10 is integrated within cluster 0\n",
    "* There's a bigger mixing of clusters 0 and 4\n",
    "\n",
    "We will study mainly the first two cases, which are more interesting.To do that, we will get the DEGs and calculate the logfold change in clusters, as well as the score (related to p-value). Then, we will compare them to the ones obtained with the DEGs using the leiden solution with triku features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_PBMC_10k.uns['leiden_colors'] = adata_sample.uns['leiden_colors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_PBMC_10k.obs['leiden'] = leiden_res_scanpy\n",
    "sc.tl.rank_genes_groups(adata_PBMC_10k, groupby='leiden', method='t-test')\n",
    "sc.pl.rank_genes_groups_tracksplot(adata_PBMC_10k, dendrogram=False, n_genes=15, palette=bold_and_vivid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_scanpy = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['scores'])\n",
    "df_DEGs_scanpy = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['names'])\n",
    "df_logfold_scanpy = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['logfoldchanges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the UMAPs and leiden clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Using corrected distances (default)\n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "sc.tl.leiden(adata_sample, resolution=0.8, random_state=seed)\n",
    "leiden_res_corrected = [i + '*' for i in adata_sample.obs['leiden']]\n",
    "adata_sample.obs['leiden'] = leiden_res_corrected\n",
    "sc.pl.umap(adata_sample, color='leiden', ax=axs[2], show=False, legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "# Default \n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.highly_variable_genes(adata_sample, n_top_genes=n_features)\n",
    "default_HVG = adata_sample.var['highly_variable'].values\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "adata_sample.obs['leiden'] = leiden_res_corrected\n",
    "sc.pl.umap(adata_sample,  color='leiden', ax=axs[0], show=False, legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "# Using uncorrected distances\n",
    "adata_sample = adata_PBMC_10k.copy()\n",
    "dist_uncorrected = adata_sample.var['triku_distance_uncorrected'] - adata_sample.var['triku_distance_random']\n",
    "cutoff = np.sort(dist_uncorrected)[-n_features]\n",
    "adata_sample.var['highly_variable'] = dist_uncorrected >= cutoff\n",
    "\n",
    "sc.pp.log1p(adata_sample)\n",
    "sc.pp.pca(adata_sample)\n",
    "sc.pp.neighbors(adata_sample, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample, random_state=seed, min_dist=0.5)\n",
    "adata_sample.obs['leiden'] = leiden_res_corrected\n",
    "sc.pl.umap(adata_sample, color='leiden', ax=axs[1], legend_loc='on data', palette=bold_and_vivid)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some peculiarities arise:\n",
    "* 3* and 9* clusters now are separated. The UMAP in scanpy's featues seems more resolved, but 3 and 6 from then are not exactly 3 and 6 from now, which is strange. Maybe it was a bad clustering solution in previous UMAPs.\n",
    "* Cluster 17* areises as a separated cluster iun triku's features.\n",
    "* Clusters 12* and 16* seem completely integrated in cluster 2 in leiden from scanpy feature selection.\n",
    "* Relationships between clusters 1\\*, 2\\*, 5\\*, 6\\*, 7\\*, 8\\*, and 11\\* become more apparent. For example, cluster 11\\* expression is clearly an intermediate between 5\\* and 8\\*, whereas this relationship is less apparent when looking at the corresponding clusters 5, 7, 8.  Also, clusters 1\\*, 2\\*, 6\\* and 7\\* seem to be more separated than 0 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_PBMC_10k.uns['leiden_colors'] = adata_sample.uns['leiden_colors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_PBMC_10k.obs['leiden'] = leiden_res_corrected\n",
    "sc.tl.rank_genes_groups(adata_PBMC_10k, groupby='leiden', method='t-test', dendrogram=False)\n",
    "sc.pl.rank_genes_groups_tracksplot(adata_PBMC_10k, dendrogram=False, n_genes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_triku = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['scores'])\n",
    "df_DEGs_triku = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['names'])\n",
    "df_logfold_triku = pd.DataFrame(adata_PBMC_10k.uns['rank_genes_groups']['logfoldchanges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_triku_scanpy(list_cols_triku, list_cols_scanpy, N):\n",
    "    list_genes = list(set(df_DEGs_triku[list_cols_triku].loc[:N].values.ravel().tolist() + \n",
    "                     df_DEGs_scanpy[list_cols_scanpy].loc[:N].values.ravel().tolist()))\n",
    "    list_logfold_max_triku, list_logfold_max_scanpy = [], []\n",
    "    list_score_max_triku, list_score_max_scanpy = [], []\n",
    "\n",
    "    for gene in list_genes:\n",
    "        df_numeric_triku = df_logfold_triku[df_DEGs_triku[list_cols_triku].loc[:] == gene].fillna(0)\n",
    "        df_numeric_scanpy = df_logfold_scanpy[df_DEGs_scanpy[list_cols_scanpy].loc[:] == gene].fillna(0)\n",
    "\n",
    "        list_logfold_max_triku.append(df_numeric_triku.values.max())\n",
    "        list_logfold_max_scanpy.append(df_numeric_scanpy.values.max())\n",
    "\n",
    "        df_numeric_triku = df_scores_triku[df_DEGs_triku[list_cols_triku].loc[:] == gene].fillna(0)\n",
    "        df_numeric_scanpy = df_scores_scanpy[df_DEGs_scanpy[list_cols_scanpy].loc[:] == gene].fillna(0)\n",
    "\n",
    "        list_score_max_triku.append(df_numeric_triku.values.max())\n",
    "        list_score_max_scanpy.append(df_numeric_scanpy.values.max())\n",
    "    \n",
    "    \n",
    "    df_max_vals_logs = pd.DataFrame({'gene': list_genes, 'max_triku': list_logfold_max_triku, \n",
    "                            'max_scanpy': list_logfold_max_scanpy})\n",
    "    df_max_vals_logs['sum'] = df_max_vals_logs['max_triku'] + df_max_vals_logs['max_scanpy']\n",
    "    df_max_vals_logs = df_max_vals_logs.sort_values(by='sum', ascending=False)\n",
    "\n",
    "    df_max_vals_score = pd.DataFrame({'gene': list_genes, 'max_triku': list_score_max_triku, \n",
    "                                'max_scanpy': list_score_max_scanpy})\n",
    "    df_max_vals_score['sum'] = df_max_vals_score['max_triku'] + df_max_vals_score['max_scanpy']\n",
    "    df_max_vals_score = df_max_vals_score.sort_values(by='sum', ascending=False)\n",
    "    \n",
    "    p = figure(x_range=df_max_vals_logs['gene'].values, plot_width=900, plot_height=300,\n",
    "               y_axis_label='Logfold change')\n",
    "\n",
    "    p.vbar(x = dodge('gene', -0.19, range=p.x_range), top = 'max_triku', width=0.175, source=df_max_vals_logs, \n",
    "           color=\"#900020\", legend_label='triku')\n",
    "    p.vbar(x = dodge('gene', 0.19, range=p.x_range), top = 'max_scanpy', width=0.175, source=df_max_vals_logs,\n",
    "          legend_label='scanpy')\n",
    "    p.xaxis.major_label_orientation = 0.85\n",
    "    show(p)\n",
    "    \n",
    "    \n",
    "    p = figure(x_range=df_max_vals_score['gene'].values, plot_width=900, plot_height=300,\n",
    "               y_axis_label='Rank gene score')\n",
    "\n",
    "    p.vbar(x = dodge('gene', -0.19, range=p.x_range), top = 'max_triku', width=0.175, source=df_max_vals_score, color=\"#900020\")\n",
    "    p.vbar(x = dodge('gene', 0.19, range=p.x_range), top = 'max_scanpy', width=0.175, source=df_max_vals_score)\n",
    "    p.xaxis.major_label_orientation = 0.85\n",
    "    show(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing scanpy's 3 and 7 clusters with triku's 4* and 8* clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red is triku, blue is scanpy\n",
    "compare_triku_scanpy(['3*', '9*'], ['3', '6'], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that scanpy's HVG detection is generally outperformed by triku's, either by logfold changes or by rank gene scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing scanpy's 4 cluster with triku's 2* cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red is triku, blue is scanpy\n",
    "compare_triku_scanpy(['2*'], ['4'], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although logfold changes are higher in scanpy, all scores are better in triku. If we examine closely the trackplots from triku and scanpy, we see that the first DEGs in cluster 4 do not show almost any expression, neither even jointly with other clusters. On the other hand, DEGs in cluster 2\\*, although  coexpressed with clusters 1\\*, 5\\*, 7\\* and 11\\*, there is a marked expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triku tells cluster 17* apart from clusters 3* and 9*, scanpy does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Using corrected distances (default)\n",
    "adata_sample_triku = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample_triku)\n",
    "sc.pp.pca(adata_sample_triku)\n",
    "sc.pp.neighbors(adata_sample_triku, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample_triku, random_state=seed, min_dist=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Default \n",
    "adata_sample_scanpy = adata_PBMC_10k.copy()\n",
    "sc.pp.log1p(adata_sample_scanpy)\n",
    "sc.pp.highly_variable_genes(adata_sample_scanpy, n_top_genes=n_features)\n",
    "sc.pp.pca(adata_sample_scanpy)\n",
    "sc.pp.neighbors(adata_sample_scanpy, n_neighbors=35)\n",
    "sc.tl.umap(adata_sample_scanpy, random_state=seed, min_dist=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_sample_triku, color=df_DEGs_triku['17*'].loc[:15].values,  legend_loc='on data', cmap=magma, ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_sample_scanpy, color=df_DEGs_triku['17*'].loc[:15].values,  legend_loc='on data', cmap=magma, ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that cluster 17\\* has a mixed expression from clusters 3\\* and 9\\*, and cluster 15\\*. Scanpy's feature selection would not have directly found that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 10 is irrelevant for analysis\n",
    "If we look at DEGs from cluster 10, all genes are RPS, which are irrelevant for the analysis, and CD8B, which is also expressed in cluster . Cluster 10 lacks expression of NKG7, HCST and CST7. Cluster 8 expression signature is really similar to one of clusters 5 and 6 and, in fact, 8 cluster merges into clusters 5\\* and 11\\*, which do have more independent signatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_DEGs_scanpy['10'][:20].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions / wrapup\n",
    "Applying the median correction seems not to be a mistake either for artificial or biological datasets. If it does not provide an improvement, it at least does not reduce the quality of the clustering or dimensionality reduction, which is good. We will do more thorough analyses in the following noteboooks to compare triku to other feature selection methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alex-base] *",
   "language": "python",
   "name": "conda-env-alex-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
